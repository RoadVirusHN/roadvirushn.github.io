<!DOCTYPE html>
<html lang="kr"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸ | The Digital garden of Nurgle.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸" />
<meta property="og:locale" content="kr" />
<meta name="description" content="style: number min_depth: 2 max_depth: 3 varied_style: true ìì—°ì–´ì²˜ë¦¬(Natural Language Processing, NLP) ê¸°ë³¸" />
<meta property="og:description" content="style: number min_depth: 2 max_depth: 3 varied_style: true ìì—°ì–´ì²˜ë¦¬(Natural Language Processing, NLP) ê¸°ë³¸" />
<link rel="canonical" href="http://localhost:4000/articles/AI/NLP/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%B3%B8.html" />
<meta property="og:url" content="http://localhost:4000/articles/AI/NLP/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%B3%B8.html" />
<meta property="og:site_name" content="The Digital garden of Nurgle." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-14T13:41:08+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-12-14T13:41:08+09:00","datePublished":"2022-12-14T13:41:08+09:00","description":"style: number min_depth: 2 max_depth: 3 varied_style: true ìì—°ì–´ì²˜ë¦¬(Natural Language Processing, NLP) ê¸°ë³¸","headline":"ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/articles/AI/NLP/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%B3%B8.html"},"url":"http://localhost:4000/articles/AI/NLP/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%B3%B8.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="The Digital garden of Nurgle." /></head>
<div class="scrollWrapper">
  <div class="scrollbar"></div>
  <div class="progressbar"></div>
  <div class="scrollbarButton"></div>
</div>

<link rel="stylesheet" href="/assets/css/obsidian/obs-scrollbar.css" />

<!--<div class="redirection">
  <h1 class="name">Redirection for full experience.</h1>
  <br>
  Move to <br /> <a class="to" href="#">netlify url</a><br />
  <div>after <span class="counter">10</span>secs.</div>
  press <button class="cancle">here</button> to cancle.
</div>
<div class="overlay"></div>
<script type="module" src="/assets/scripts/common/components/init_redirection.js"></script>

<link rel="stylesheet" href="/assets/css/common/redirection.css" />-->

<body><header class="site-header" role="banner">

  <div class="wrapper" style="display: flex; justify-content: space-between;"><div id="header-wrapper">
    <a class="site-title" rel="author" href="/blog">The Digital garden of Nurgle.</a>

    </div><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><script src="https://unpkg.com/lunr/lunr.js"></script>
<link rel="stylesheet" href="/assets/css/common/searchbar.css" />

<form id="search-form" method="get">
  <span id="search-wrapper">
    <span id="tag-holder" ></span>
    <input type="text" id="search-box" placeholder='Prefix "#" to add Tag.' autocomplete="off">
    <span class="inner-search" >ğŸ”</span>
  </span>
</form><a class="page-link" href="/">ABOUT ME</a><a class="page-link" href="/blog">ALL ARTICLES</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
    <link rel="stylesheet" href="/assets/css/common/drawer.css" />
<button class="drawer-button open">â–¶ï¸</button>
<div id="drawer" class="close">
  <button class="drawer-button close">
    â—€ï¸
  </button>
  <div class="drawer-content">
    <div class="my-description">
      <div class="avatar-section" style="display: flex; flex-direction: row;">

        <img src="/assets/img/common/avatar.png" alt="avatar" class="avatar">
        <div style="display: flex; flex-direction: column; margin-left: 5px;">
          <a href="/about/">
            <h3 class="name">ROADVIRUSHN</h3>
          </a>
          <div class="stack-list" style="margin: 5px 0 0 5px;">
            <a title="My github page" href="https://github.com/RoadVirusHN">
  <svg class="svg-icon" width="16" height="16" viewBox="0 0 16 16">
    <use xlink:href="/assets/svg/social-icons.svg#github"></use>
  </svg>
</a>
<a title="My G-mail" href="mailto:roadvirushn@gmail.com">
  <svg class="svg-icon" width="16" height="16" viewBox="0 0 16 16">
    <use xlink:href="/assets/svg/social-icons.svg#gmail"></use>
  </svg>
</a>
<a title="My Blog" href="https://luminous-bubblegum-8e9be4.netlify.app">
  <svg class="svg-icon" width="16" height="16" viewBox="0 0 16 16">
    <use xlink:href="/assets/svg/social-icons.svg#blog"></use>
  </svg>
</a>
          </div>
        </div>
        <!-- <h4 class="name">(JUNSEOK YUN)</h4> -->
      </div>
      <p style="margin: 5px 0 0 0;">
        í’€ìŠ¤íƒ ì›¹ğŸŒ ê°œë°œì ì§€ë§ìƒ ğŸ§‘ğŸ½â€ğŸ’»
        <br>
        â• ì¸ê³µì§€ëŠ¥ ê´€ì‹¬ ğŸ¤–
      </p>
    </div>
      <hr>
      <div class="categories">
        <h3 style="margin: 0;"><a href="/">Categories</a></h3>
        <ul class="category-list">
  
  
  
  <li>
    <strong style="font-size: larger;">â”£ </strong>
    <h3 style="display: inline;">
      
      <span class="category-drop-down">â–¶</span>
      
      <a class="category-link" href="/categories/COMPUTER_SCIENCE/" >COMPUTER_SCIENCE</a>
    </h3>
    <span style="font-size: xx-small;">
       
      ğŸ“‚: 5
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          â”ƒ  
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/DATABASE/">DATABASE</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          â”ƒ  
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/ALGORITHM/">ALGORITHM</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 15 
            ğŸ“‚: 1
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          â”ƒ  
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/OS/">OS</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 14 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          â”ƒ  
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/NETWORK/">NETWORK</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 8 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          â”ƒ  
          â”— 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/COMPUTER_SCIENCE/ETC/">ETC</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 1 
            
          </span>
      </li>
      
    </ul>
  </li>
  
  
  <li>
    <strong style="font-size: larger;">â”£ </strong>
    <h3 style="display: inline;">
      
      <span class="category-drop-down">â–¶</span>
      
      <a class="category-link" href="/categories/WEB/" >WEB</a>
    </h3>
    <span style="font-size: xx-small;">
       
      ğŸ“‚: 3
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          â”ƒ  
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/WEB/FRONTEND/">FRONTEND</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 2 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          â”ƒ  
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/WEB/BACKEND/">BACKEND</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 5 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          â”ƒ  
          â”— 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/WEB/CI,CD/">CI,CD</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 2 
            ğŸ“‚: 2
          </span>
      </li>
      
    </ul>
  </li>
  
  
  <li>
    <strong style="font-size: larger;">â”£ </strong>
    <h3 style="display: inline;">
      
      <a class="category-link" href="/categories/ETC/" >ETC</a>
    </h3>
    <span style="font-size: xx-small;">
      ğŸ“„: 9 
      
    </span>
    <ul class="child-category-list">
      
    </ul>
  </li>
  
  
  <li>
    <strong style="font-size: larger;">â”— </strong>
    <h3 style="display: inline;">
      
      <span class="category-drop-down">â–¶</span>
      
      <a class="category-link" href="/categories/AI/" >AI</a>
    </h3>
    <span style="font-size: xx-small;">
       
      ğŸ“‚: 9
    </span>
    <ul class="child-category-list">
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/AITOOLS/">AITOOLS</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 3 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/CV/">CV</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 2 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/DEEP_LEARNING/">DEEP_LEARNING</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/DATA_VIS/">DATA_VIS</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 2 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/GRAPH/">GRAPH</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/LIGHTWEIGHT/">LIGHTWEIGHT</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/MATH/">MATH</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 1 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          â”£ 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/NLP/">NLP</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 3 
            
          </span>
      </li>
      
      <li>
        <strong style="font-size: larger;">
          &nbsp;&nbsp;&nbsp;
          â”— 
        </strong>
        <h4 style="display: inline; background-color: lightgray; padding: 0px 1px; border-radius: 2px;">
          <a class="category-link" href="/categories/AI/STRUCTURED_DATA/">STRUCTURED_DATA</a>
        </h4>
          <span style="font-size: xx-small;">
            ğŸ“„: 2 
            
          </span>
      </li>
      
    </ul>
  </li>
  
</ul>
      </div>
      <hr>
      <div class="recent-view">
        <h3 style="margin: 0;">Recent views</h3>
        <ul style="margin: 0;">
          <li>
            <strong style="color:rgb(219, 219, 12);">1 <a id="recent-1"></a></strong>
          </li>
          <li>
            2 <a id="recent-2"></a>
          </li>
          <li>
            3 <a id="recent-3"></a>
          </li>
          <li>
            4 <a id="recent-4"></a>
          </li>
          <li>
            5 <a id="recent-5" style="overflow: hidden;"></a>
          </li>
        </ul>
      </div>
    </div>
    <hr>
  <div style="height: 7vh;"></div>
</div>
    <div class="wrapper">
      <article class="article h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="article-header">
    <h1 class="article-title a-name" itemprop="name headline">ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸</h1>
    <p class="article-meta">
      <time class="dt-published" datetime="2022-12-14T13:41:08+09:00" itemprop="datePublished">Dec 14, 2022
      </time></p>
  </header>

  <div class="article-content e-content" itemprop="articleBody">
     
  
<script>
  MathJax = {
    tex: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"],
      ],
    },
    svg: {
      fontCache: "global",  
     // scale: 1.5,
    },
    chtml: {
     // scale: 1.5,
    },
  };
</script>
<script
  type="text/javascript"
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"
></script>
 
 


<script src="/assets/scripts/bundle/obsidian.bundle.js"></script>
<link rel="stylesheet" href="/assets/css/obsidian/callout.css" />
<link rel="stylesheet" href="/assets/css/obsidian/image.css" />
<link rel="stylesheet" href="/assets/css/obsidian/link-warning.css" />
<link rel="stylesheet" href="/assets/css/obsidian/preview.css" />

<div class="content-section">
  
<ol id='markdown-toc-0'>
<li><a href='#Intro-to-Natural-Language-Processing-NLP' id='markdown-toc-0-Intro-to-Natural-Language-Processing-NLP'>Intro to Natural Language Processing(NLP)</a><ul><li><a href='#NLPì˜-íŠ¸ëœë“œ' id='markdown-toc-0-NLPì˜-íŠ¸ëœë“œ'>NLPì˜ íŠ¸ëœë“œ</a></li> 
<li><a href='#Bag-of-Words-and-NaiveBayes-Classifier' id='markdown-toc-0-Bag-of-Words-and-NaiveBayes-Classifier'>Bag-of-Words and NaiveBayes Classifier</a><ul>
</ul></li> 
<li><a href='#Word-Embedding-Word2Vec-GloVe' id='markdown-toc-0-Word-Embedding-Word2Vec-GloVe'>Word Embedding: Word2Vec, GloVe</a><ul>
</ul></li> 

</ul></li> 
<li><a href='#Recurrent-Neural-Networks-RNNs' id='markdown-toc-0-Recurrent-Neural-Networks-RNNs'>Recurrent Neural Networks(RNNs)</a><ul><li><a href='#Basics-of-Recurrent-Neural-Networks-RNNs' id='markdown-toc-0-Basics-of-Recurrent-Neural-Networks-RNNs'>Basics of Recurrent Neural Networks(RNNs)</a></li> 
<li><a href='#Types-of-RNNs' id='markdown-toc-0-Types-of-RNNs'>Types of RNNs</a></li> 
<li><a href='#Character-level-Language-Model' id='markdown-toc-0-Character-level-Language-Model'>Character-level Language Model</a><ul>
</ul></li> 
<li><a href='#Long-Short-Term-Memory-LSTM-and-Gated-Recurrent-Unit-GRU' id='markdown-toc-0-Long-Short-Term-Memory-LSTM-and-Gated-Recurrent-Unit-GRU'>Long Short-Term Memory(LSTM) and Gated Recurrent Unit(GRU)</a><ul>
</ul></li> 

</ul></li> 
<li><a href='#Sequence-to-Sequence-with-Attention' id='markdown-toc-0-Sequence-to-Sequence-with-Attention'>Sequence to Sequence with Attention</a><ul><li><a href='#Seq2Seq-amp-Encoder-decoder-amp-attention' id='markdown-toc-0-Seq2Seq-amp-Encoder-decoder-amp-attention'>Seq2Seq &amp; Encoder-decoder &amp; attention</a><ul>
</ul></li> 
<li><a href='#Beam-search' id='markdown-toc-0-Beam-search'>Beam search</a><ul>
</ul></li> 
<li><a href='#BLEU-score' id='markdown-toc-0-BLEU-score'>BLEU score</a><ul>
</ul></li> 

</ul></li> 
<li><a href='#Tansformer' id='markdown-toc-0-Tansformer'>Tansformer</a><ul><li><a href='#Transform-introduction' id='markdown-toc-0-Transform-introduction'>Transform introduction</a></li> 
<li><a href='#Transformer-contâ€™d' id='markdown-toc-0-Transformer-contâ€™d'>Transformer(contâ€™d)</a><ul>
</ul></li> 

</ul></li> 
<li><a href='#Self-supervised-Pre-training-Models' id='markdown-toc-0-Self-supervised-Pre-training-Models'>Self-supervised Pre-training Models</a><ul><li><a href='#Self-Supervised-Pre-Training-Models' id='markdown-toc-0-Self-Supervised-Pre-Training-Models'>Self-Supervised Pre-Training Models</a><ul>
</ul></li> 
<li><a href='#Advanced-Self-supervised-Pre-training-Models' id='markdown-toc-0-Advanced-Self-supervised-Pre-training-Models'>Advanced Self-supervised Pre-training Models</a><ul>
</ul></li> 

</ul></li> 
</ol>
<h1 id='ìì—°ì–´ì²˜ë¦¬-Natural-Language-Processing-NLP-ê¸°ë³¸'>ìì—°ì–´ì²˜ë¦¬(Natural Language Processing, NLP) ê¸°ë³¸</h1>

<blockquote>
  <p>NAVER AI boost camp ìˆ˜ì—…ì„ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.</p>
</blockquote>

<h2 id='Intro-to-Natural-Language-Processing-NLP'>Intro to Natural Language Processing(NLP)</h2>

<ul>
  <li>ì»´í“¨í„°ê°€ ì£¼ì–´ì§„ ë‹¨ì–´ë‚˜ ë¬¸ì¥, ë¬¸ë‹¨, ê¸€ì„ ì´í•´í•˜ëŠ” NLU(Natural Language Understading)</li>
  <li>
    <p>ì´ëŸ° í•œ ê¸€ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” NLG(Natural Language Generation)ë¡œ ì´ë£¨ì–´ì§</p>
  </li>
  <li>NLPì˜ ì˜ì—­(ACL, EMNLP, NAACL ë“±ì—ì„œ ì—°êµ¬)
    <ul>
      <li>ìš°ë¦¬ê°€ ì£¼ë¡œ ë‹¤ë£° ë¶„ì•¼</li>
      <li>Low-level parsing
        <ul>
          <li>Tokenization: ë¬¸ì¥ì„ ì´ë£¨ëŠ” ê° ë‹¨ì–´ë¥¼ ì •ë³´ ë‹¨ìœ„(Token)ë¡œ ìª¼ê°œë‚˜ê°€ëŠ” ê²ƒ</li>
          <li>stemming: ë‹¨ì–´ì˜ ë‹¤ì–‘í•œ í‘œí˜„ ë³€í˜•ì„ ì—†ì• ê³  ì˜ë¯¸ë§Œ ë‚¨ê¸°ëŠ” ê²ƒ
            <ul>
              <li>ë§‘ê³ , ë§‘ì§€ë§Œ, ë§‘ì•˜ëŠ”ë°, ë§‘ì€ = ë§‘ë‹¤.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>word and phrase level
        <ul>
          <li>Named entity recognition(NER) : ê³ ìœ  ëª…ì‚¬ ì¸ì‹
            <ul>
              <li>(Newyork timesëŠ” newyork + timesê°€ ì•„ë‹ˆë‹¤)</li>
            </ul>
          </li>
          <li>part-of-speech(POS) tagging : ë‹¨ì–´ì˜ í’ˆì‚¬, ì„±ë¶„ì´ ë¬´ì—‡ì¸ê°€?(ëª…ì‚¬, ëª©ì ì–´, í˜•ìš©ì‚¬ ë“±)</li>
        </ul>
      </li>
      <li>Senetence level
        <ul>
          <li>ë¬¸ì¥ì˜ ê°ì •ë¶„ì„, ê¸°ê³„ë²ˆì—­</li>
        </ul>
      </li>
      <li>Multi-sentence and paragraph level
        <ul>
          <li>Entailment prediction : ë‘ ë¬¸ì¥ì˜ ëª¨ìˆœê´€ê³„, ë…¼ë¦¬ì  ë‚´í¬ë¥¼ í™•ì¸
            <ul>
              <li>ë¬¸ì¥ì´ ì•ì„œ í–ˆë˜ ë§ê³¼ ë‹¤ë¥¸ê°€?</li>
            </ul>
          </li>
          <li>ì§ˆì˜ ì‘ë‹µ, ë¬¸ì„œ ìš”ì•½</li>
          <li>dialog systems: ì±—ë´‡</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Text miningì˜ ì˜ì—­(KDD, WSDM, WWW ë“±ì—ì„œ ì—°êµ¬ì¤‘)
    <ul>
      <li>íŠ¸ëœë“œ ë¶„ì„, ìƒí’ˆ ë°˜ì‘ ë¶„ì„, SNS ì‚¬íšŒê³¼í•™ ë¶„ì„ ë“±</li>
    </ul>
  </li>
  <li>Information retrieval(ê²€ìƒ‰ ê¸°ìˆ )
    <ul>
      <li>ë§ì´ ì„±ìˆ™í•´ì§„ ìƒíƒœ, ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼,</li>
      <li>ì¶”ì²œ ì‹œìŠ¤í…œ, ê°œì¸í™” ê´‘ê³  ë“±</li>
    </ul>
  </li>
</ul>

<h3 id='NLPì˜-íŠ¸ëœë“œ'>NLPì˜ íŠ¸ëœë“œ</h3>

<ul>
  <li>ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ì„ vectorë¡œ í‘œí˜„í•˜ì—¬ ê·¸ë˜í”„ ë‚´ì˜ ì ìœ¼ë¡œ ë°”ê¾¸ì–´ ì²˜ë¦¬í•˜ê²Œ ë¨(Word Embedding).</li>
  <li>ìì—°ì–´ ì²˜ë¦¬ì—ì„œ RNN ëª¨ë¸ì„ ìœ„í•œ LSTM, GRU ìœ ë‹›ì´ ì‚¬ìš©ë˜ë‹¤ ìµœê·¼ êµ¬ê¸€ì˜ attention moduleê³¼ Transformer modelì˜ ë°œí‘œë¡œ ì„±ëŠ¥ì´ í¬ê²Œ ëŠ˜ì–´ë‚¨.</li>
  <li>ë”ì´ìƒ rule base ê¸°ê³„ ë²ˆì—­ì€ ë”¥ëŸ¬ë‹ base ê¸°ê³„ë²ˆì—­ì— ì„±ëŠ¥ìœ¼ë¡œ ë’¤ì³ì§€ê³  ìˆìŒ.</li>
  <li>ìµœê·¼ì—ëŠ” ìê°€ ì§€ë„ í•™ìŠµ ë˜ëŠ” pretrained modelì„ í†µí•˜ì—¬ ë” ì´ìƒ labelingì´ í•„ìš”í•˜ì§€ ì•Šì€ BERT, GPT-3 ê°™ì€ ê¸°ìˆ ì´ ë‚˜íƒ€ë‚¨.</li>
  <li>ë‹¤ë§Œ ì´ëŸ¬í•œ ê¸°ìˆ  ë“¤ì€ ëŒ€ê·œëª¨ì˜ ì»´í“¨íŒ… ëŠ¥ë ¥ê³¼ ë°ì´í„°ê°€ í•„ìš”í•¨.</li>
</ul>

<h3 id='Bag-of-Words-and-NaiveBayes-Classifier'>Bag-of-Words and NaiveBayes Classifier</h3>

<ul>
  <li>Bag-of-WordsëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì´ì „ ì‹œëŒ€ ë•Œ, ë‹¨ì–´ì™€ ë¬¸ì„œë¥¼ ìˆ«ìí˜•íƒœë¡œ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ ê¸°ë²•</li>
  <li>NaiveBayes Classifier : Bag-of-Words ë°©ì‹ì„ ì´ìš©í•œ ì „í†µì ì¸ ë¬¸ì„œ ë¶„ë¥˜ ê¸°ë²•</li>
</ul>

<h4 id='Bag-of-Words-Representation'>Bag-of-Words Representation</h4>

<ul>
  <li>
    <p>Bag-of-Wordsì˜ ê³¼ì •</p>

    <ol>
      <li>ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ë‹¨ì–´ê°€ ê°ê° í¬í•¨ë˜ì–´ìˆëŠ” ì‚¬ì „(vocabulary) ìƒì„±
        <ul>
          <li>â€œJon really really loves this movieâ€, â€œJane really likes this songâ€ì—ì„œ</li>
          <li>Vocabulary: {â€œJohnâ€, â€œreallyâ€, â€œlovesâ€,  â€œthisâ€,â€movieâ€,â€Janeâ€,â€likesâ€,â€songâ€} ìƒì„±</li>
        </ul>
      </li>
      <li>ì‚¬ì „ì— í¬í•¨ëœ ìœ ì¼í•œ ë‹¨ì–´ë“¤ì„ one-hot vectorë¡œ ì¸ì½”ë”©
        <ul>
          <li>Vocabulary: {â€œJohnâ€, â€œreallyâ€, â€œlovesâ€,  â€œthisâ€,â€movieâ€,â€Janeâ€,â€likesâ€,â€songâ€}</li>
          <li>ì°¨ë¡€ëŒ€ë¡œ one-hot vector í˜•ì‹ìœ¼ë¡œ ì¸ì½”ë”©
            <ul>
              <li>ì˜ˆë¥¼ ë“¤ì–´  John:[1 0 0 0 0 0 0 0], really: [0 1 0 0 0 0 0 0] â€¦ song: [0 0 0 0 0 0 0 1]</li>
              <li>word- embedding ë°©ë²•ê³¼ì˜ ì°¨ì´ì </li>
            </ul>
          </li>
          <li>ëª¨ë“  vectorìŒì˜ ê±°ë¦¬ëŠ” $\sqrt 2$, cosine similarity ëŠ” 0ìœ¼ë¡œ ê³„ì‚°ëœë‹¤.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p>ì´ë ‡ê²Œ êµ¬ì„±ëœ ë¬¸ì¥ì´ë‚˜ ë‹¨ì–´ëŠ” one-hot vectorë“¤ì˜ í•©ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

    <ul>
      <li>ì˜ˆì‹œ 1. â€œJohn really really loves this movieâ€
        <ul>
          <li>John + really + really + loves + this + movie: [1 2 1 1 1 0 0 0]</li>
        </ul>
      </li>
      <li>ì˜ˆì‹œ 2. â€œJane really likes this songâ€
        <ul>
          <li>Jane + really + likes + this + song: [0 1 0 1 0 1 1 1]</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id='NaiveBayes-Classifier-for-Document-Classification'>NaiveBayes Classifier for Document Classification</h4>

<ul>
  <li>NaiveBayes Classifierë€, Bag of words ë°©ë²•ìœ¼ë¡œ í‘œí˜„ëœ ë¬¸ì„œë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ì „í†µì ì¸ ë°©ë²•</li>
  <li>dê°œì˜ ë¬¸ì„œì™€ Cì˜ ë¬¸ì„œë¶„ë¥˜ ì§‘í•©ì´ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , c ë¶„ë¥˜ëŠ” C ë¬¸ì„œë¶„ë¥˜ ì§‘í•©ì˜ ì›ì†Œì¼ ë•Œ, ê°ê°ì˜ í´ë˜ìŠ¤ì— dë¬¸ì„œê°€ ì†í•  í™•ë¥  ë¶„í¬ëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ëœë‹¤.</li>
</ul>

\[C_{MAP}=argmax\ P(c|d) :"maximum\ a\ posteriori" = most\ likely\ class \\ = argmax\ \frac{P(d|c)P(c)}{P(d)}: Bayes\ Rule \\ = argmax\ P(d|c)P(c):Dropping\ the\ denominator\\
where\ c \in C\]

<p><strong>[math. Bayersâ€™ Rule Applied to Documents and Classes ]</strong></p>

<ul>
  <li>
    <p>P(d)ëŠ” d documentê°€ ë½‘í í™•ë¥  ì´ë©°, ê³ ì •ëœ ê°’ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆì–´ì„œ ë¬´ì‹œí•  ìˆ˜ ìˆìŒ</p>
  </li>
  <li>
    <p>ë‹¨ì–´ë“¤ wë¡œ ì´ë£¨ì–´ì§„ c ë¶„ë¥˜ì˜ d ë¬¸ì„œì—ì„œ íŠ¹ì • ë¶„ë¥˜ cê°€ ê³ ì •ì¼ ë•Œ, ë¬¸ì„œ dê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì€</p>
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$P(d</td>
              <td>c)P(c) = P(w1,w2,\dots,w_n</td>
              <td>c)P(c)\rightarrow P(c)\prod_{w_i\in W}P(w_i</td>
              <td>c)$</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>ê° ë‹¨ì–´(w~1~,w~2~,â€¦,w~n~)ê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì„ ë…ë¦½ì ì´ë¼ê³  ê°€ì •</li>
    </ul>
  </li>
</ul>

<h5 id='NaiveBayes-ë¶„ë¥˜-ì˜ˆì‹œ'>NaiveBayes ë¶„ë¥˜ ì˜ˆì‹œ</h5>

<table>
  <thead>
    <tr>
      <th>Â </th>
      <th>Doc(d)</th>
      <th>Document(words, w)</th>
      <th>Class(c)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Training</td>
      <td>1</td>
      <td>Image recognition uses convolutional neural networks</td>
      <td>CV</td>
    </tr>
    <tr>
      <td>Â </td>
      <td>2</td>
      <td>Transformer can be use for image classification task</td>
      <td>CV</td>
    </tr>
    <tr>
      <td>Â </td>
      <td>3</td>
      <td>Language modeling uses transformer</td>
      <td>NLP</td>
    </tr>
    <tr>
      <td>Â </td>
      <td>4</td>
      <td>Document classification task is language task</td>
      <td>NLP</td>
    </tr>
    <tr>
      <td>Test</td>
      <td>5</td>
      <td>Classification task uses transformer</td>
      <td>?</td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. NaiveBayes ì˜ˆì‹œ]</strong></p>

<ul>
  <li>ìœ„ í‘œì˜ ìƒí™©ì—ì„œ classì˜ ì†í•  í™•ë¥ ì€
    <ul>
      <li>$P(c_{cv})=\frac{2}{4}=\frac{1}{2}$</li>
      <li>$P(c_{NLP}) = \frac {2}{4}=\frac {1}{2}$</li>
    </ul>
  </li>
  <li>ì´ë©°, ì´ë•Œ ê° ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì€</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(w_k</td>
          <td>c_i)=\frac {n_k}{n}$, where n~k~ is occurreneces of w~k~ in documents of topic c~i~</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Word</th>
      <th>Prob</th>
      <th>Word</th>
      <th>Prob</th>
      <th>Â </th>
      <th>Â </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$P(w_{â€œclassificationâ€}</td>
      <td>c_{CV})$</td>
      <td>$\frac {1}{14}$</td>
      <td>$P(w_{â€œclassificationâ€}</td>
      <td>c_{NLP})$</td>
      <td>$\frac {1}{10}$</td>
    </tr>
    <tr>
      <td>$P(w_{â€œtaskâ€}</td>
      <td>c_{CV})$</td>
      <td>$\frac {1}{14}$</td>
      <td>$P(w_{â€œtaskâ€}</td>
      <td>c_{NLP})$</td>
      <td>$\frac {2}{10}$</td>
    </tr>
    <tr>
      <td>$P(w_{â€œusesâ€}</td>
      <td>c_{CV})$</td>
      <td>$\frac {1}{14}$</td>
      <td>$P(w_{â€œusesâ€}</td>
      <td>c_{NLP})$</td>
      <td>$\frac {1}{10}$</td>
    </tr>
    <tr>
      <td>$P(w_{â€œtransformerâ€}</td>
      <td>c_{CV})$</td>
      <td>$\frac {1}{14}$</td>
      <td>$P(w_{â€œtransformerâ€}</td>
      <td>c_{NLP})$</td>
      <td>$\frac {1}{10}$</td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. ê° ë‹¨ì–´ê°€ class ë‚´ì— ë‚˜íƒ€ë‚  í™•ë¥ ë“¤]</strong></p>

<ul>
  <li>document d~5~=â€Classification task uses transformerâ€ë¥¼ í†µí•˜ì—¬ classì— ì†í•  í™•ë¥ ì„ êµ¬í•´ë³´ë©´
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$P(c_{CV}</td>
              <td>d_5)=P(c_{CV})\prod_{w\in W}P(w</td>
              <td>c_{CV})=\frac{1}{2}\times \frac{1}{14}\times \frac{1}{14}\times \frac{1}{14}\times \frac{1}{14}\times = 0.000013$</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$P(c_{NLP}</td>
              <td>d_5)=P(c_{NLP})\prod_{w\in W}P(w</td>
              <td>c_{NLP})=\frac{1}{2}\times \frac{1}{10}\times \frac{2}{10}\times \frac{1}{10}\times \frac{1}{10} = 0.0001$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>ì—¬ê¸°ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì¸ ë¶„ë¥˜ë¥¼ ë½‘ì•„ í™•ì •í•˜ê²Œ ëœë‹¤(argmax).</li>
  <li>ë‹¤ë§Œ í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë  ê²½ìš° ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì´ ìˆë”ë¼ë„ í™•ë¥ ì´ 0ì´ ë˜ë²„ë¦¬ë¯€ë¡œ regularization ê°™ì€ ë‹¤ë¥¸ ë°©ë²•ì„ ê°•êµ¬í•´ì•¼í•¨.</li>
</ul>

<h3 id='Word-Embedding-Word2Vec-GloVe'>Word Embedding: Word2Vec, GloVe</h3>

<ul>
  <li>Word2Vecê³¼ GloVeëŠ” í•˜ë‚˜ì˜ ì°¨ì›ì— ë‹¨ì–´ì˜ ëª¨ë“  ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ëŠ” one-hot-encodingê³¼ ë‹¬ë¦¬ ë‹¨ì–´ì˜ distributed representationì„ í•™ìŠµí•˜ê³ ì ê³ ì•ˆëœ ëª¨ë¸.</li>
  <li>ë¬¸ì¥ì„ ë‹¨ì–´ë¼ëŠ” ë‹¨ìœ„ ì •ë³´ë¡œ ì´ë£¨ì–´ì§„ sequence dataë¼ê³  ê°€ì •í–ˆì„ ë•Œ, ê° ë‹¨ì–´ë“¤ì„ ê³µê°„ìƒì˜ í•œ ì ì´ë‚˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ ì„ Word Embeddingì´ë¼ê³  í•œë‹¤.
    <ul>
      <li>ì˜ˆë¥¼ ë“¤ì–´ catê³¼ kittyëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ë¯€ë¡œ ë¹„ìŠ·í•œ ì¢Œí‘œê°’ì´ë‚˜ ë²¡í„°ë¥¼ ê°€ì§€ê³  ìˆê² ì§€ë§Œ, hamburgerëŠ” ì „í˜€ ë‹¤ë¥¸ ì¢Œí‘œë‚˜ ë²¡í„°ì— ìˆì„ ê²ƒì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<h4 id='Word2Vec'>Word2Vec</h4>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210215225944488.png" alt="" /></p>

<p><strong>[img. Word2Vec ì˜ ì˜ˆì‹œ]</strong></p>

<ul>
  <li>ê°™ì€ ë¬¸ì¥ì— ìì£¼ í¬í•¨ë˜ëŠ” ë‹¨ì–´ë“¤ì€ ê´€ë ¨ì„±ì´ ë§ë‹¤ëŠ” ê°€ì • í•˜ì—  Word Embeddingì„ ì§„í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
    <ul>
      <li>ì˜ˆë¥¼ ë“¤ì–´ â€œThe furry cat hunts mice.â€ ì—ì„œ catì€ furryí•˜ê³  miceë¥¼ hunts í•˜ë¯€ë¡œ ì—°ê´€ì´ ìˆë‹¤.</li>
    </ul>
  </li>
  <li>ë¬¸ì¥ ë‚´ì˜ ë‹¨ì–´ wê°€ ë‚˜íƒ€ë‚  ë•Œ, ê·¸ ì£¼ìœ„ì— ê°ê°ì˜ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚  í™•ë¥ ë¶„í¬ë¥¼ êµ¬í•˜ì—¬ ì´ìš©í•œë‹¤.</li>
  <li>Skipgram ë°©ë²•ê³¼ CBOW ë°©ë²• ë‘ê°€ì§€ë¡œ ë‚˜ë‰¨ (ì‹¤ìŠµ 2_word2vec.ipynb) ì°¸ì¡°</li>
</ul>

<h5 id='Word2Vec-Algorithmê³¼-ì˜ˆì œ'>Word2Vec Algorithmê³¼ ì˜ˆì œ</h5>

<ol>
  <li>ì£¼ì–´ì§„ ë¬¸ì¥ì„ Tokenizationí•œ í›„, ì‚¬ì „(Vocabulary) ìƒì„±í•˜ê³  ë‹¨ì–´ë³„ one-hot ë²¡í„° ìƒì„±
    <ul>
      <li>Sentence: â€œI study math.â€</li>
      <li>Vocabulary: {â€œIâ€, â€œstudyâ€, â€œmathâ€}</li>
      <li>Input: â€œstudyâ€ [0, 1, 0]</li>
      <li>Output: â€œmathâ€ [0, 0, 1]</li>
    </ul>
  </li>
  <li>Sliding Window ê¸°ë²•ì„ ì´ìš©í•´ ì• ë’¤ë¡œ ë‚˜íƒ€ë‚œ ë‹¨ì–´ë“¤ì˜ ìŒë“¤ë¡œ í•™ìŠµ ë°ì´í„° êµ¬ì„±.</li>
</ol>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216024706818.png" alt="" /></p>

<p><strong>[img. Window sizeê°€ 1ì¸ Sliding Window ê¸°ë²•ì˜ ê·¸ë¦¼]</strong></p>

<ol>
  <li>
    <p>2 layer êµ¬ì„±ì˜ ì‹¬ì¸µ ì‹ ê²½ë§ì„ í†µí•´ word embedding í•œë‹¤.</p>

    <ul>
      <li>
        <p>Input layer, output layer ì°¨ì› ìˆ˜ : vocabulary ë‹¨ì–´ ìˆ˜(one-hot vector ì°¨ì›)</p>
      </li>
      <li>
        <p>hidden layer ì°¨ì› ìˆ˜ : hyper parameter(word embedding ì°¨ì› ìˆ˜)</p>
      </li>
    </ul>

    <p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216005047259.png" alt="" /></p>

    <p><strong>[img. 2 layer ì‹ ê²½ë§ì˜ word embedding ì›ë¦¬]</strong></p>

    <ul>
      <li>input vector â€œstudyâ€ì˜ ê²½ìš°, [0,1,0]ì˜ ë²¡í„°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, hidden layerì˜ ì°¨ì›ìˆ˜ê°€ 2ë¼ê³  ê°€ì •í•  ë•Œ.</li>
      <li>linear transform matrix W~1~ì˜ ê²½ìš° 3ì°¨ì›ì˜ ë²¡í„°ë¥¼ 2ì°¨ì›ì˜ ë²¡í„°ë¡œ ë°”ê¿”ì•¼ í•˜ë¯€ë¡œ 2X3 ì°¨ì›ì„ ê°€ì§„ë‹¤.
        <ul>
          <li>W~1~ì˜ input vectorê°€ one-hot vector ì´ë¯€ë¡œ ë‚´ì ì„ êµ¬í•˜ê¸° ë³´ë‹¨ one-hot vectorì˜ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” W~1~ì˜ Columnì„ ê°€ì ¸ì˜¤ëŠ” í˜•ì‹ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.</li>
        </ul>
      </li>
      <li>linear transform matrix W~2~ì˜ ê²½ìš° ë‹¤ì‹œ 3ì°¨ì›ì˜ ë²¡í„°ë¥¼ ê°€ì ¸ì•¼ í•˜ë¯€ë¡œ 3X2 ì°¨ì›ì„ ê°€ì§„ë‹¤.
        <ul>
          <li>softmax í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œí‚¤ê¸° ì „ì˜ ì´ìƒì ì¸ logicê°’ì€ ground-truthì˜ ë‚´ì ê°’ì€ $\infin$, ê·¸ ì´ì™¸ì˜ ë‚´ì  ê°’ì€ $-\infin$ì´ ë˜ì–´ì•¼ ê²°ê³¼ê°’ë„ one-hot vectorì˜ í˜•íƒœë¡œ ë‚˜ì˜¨ë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>softmax í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œì¼œ word embedding ê°’ì„ ê°€ì ¸ì˜¨ë‹¤.</p>
  </li>
</ol>

<hr />

<ul>
  <li>ì´ë¥¼ í†µí•˜ì—¬ ì˜ë¯¸ë¡ ì  ê´€ì ì—ì„œ ë‹¨ì–´ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì•Œ ìˆ˜ ìˆëŠ” word vectorë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216020142926.png" alt="" /></p>

<p><strong>[img. ë‹¨ì–´ ê°„ì˜ ê´€ê³„ê°€ ë¹„ìŠ·í•˜ë©´ ë‘ ë‹¨ì–´ì˜ ë²¡í„°ì˜ ë°©í–¥ë„ ìœ ì‚¬í•˜ë‹¤.]</strong></p>

<ul>
  <li>https://ronxin.github.io/wevi/ ì—ì„œ Word2Vecì„ ì‹œì—°í•´ë³¼ ìˆ˜ ìˆë‹¤.</li>
  <li>https://word2vec.kr/search/ ì—ì„œ í•œêµ­ì–´ Word2Vec ê²°ê³¼ê°’ì„ ì•Œì•„ë³¼ ìˆ˜ ìˆë‹¤.</li>
  <li>Word2Vecìœ¼ë¡œ ë¬¸ë§¥ì— ì–´ìƒ‰í•œ ë‹¨ì–´ë¥¼  ì°¾ì•„ë‚´ëŠ” Intrusion Detection ë˜í•œ ê°€ëŠ¥í•˜ë‹¤.</li>
  <li>Word2Vecì„ ì´ìš©í•´ ë‹¤ìŒê³¼ ê°™ì€ ë¶„ì•¼ì— í™œìš©í•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.
    <ul>
      <li>ê¸°ê³„ë²ˆì—­</li>
      <li>PoS tagging</li>
      <li>ê³ ìœ ëª…ì‚¬ íƒœê¹…</li>
      <li>ê°ì • ë¶„ì„</li>
      <li>Image Captioning</li>
      <li>ê¸°íƒ€ ë“±ë“±</li>
    </ul>
  </li>
</ul>

<h4 id='GloVe-Another-Word-Embedding-Model'>GloVe: Another Word Embedding Model</h4>

<ul>
  <li>
    <p>Global Vectors for Word Representation</p>
  </li>
  <li>Word2Vecê³¼ í•¨ê»˜ ë§ì´ ì“°ì´ëŠ” Word Embedding ë°©ë²•</li>
  <li>ì¹´ìš´íŠ¸ ê¸°ë°˜ ë°©ë²•ë¡ (LSA)ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì˜ ë°©ë²•ë¡ (Word2Vec) ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ 
    <ul>
      <li>(ì…ë ¥ì–´ì˜ ì„ë² ë”© ë²¡í„°ì™€ ì¶œë ¥ì–´ì˜ ì„ë² ë”© ë²¡í„°ì˜ ë‚´ì ê°’)ê³¼ (ìœˆë„ìš°ì—ì„œ ë‘ ë‹¨ì–´ i, jì˜ ë™ì‹œ ì¶œì—° ë¹ˆë„ì— logë¥¼ ì”Œìš´ ê²ƒ)ì„ loss í•¨ìˆ˜ë¡œì¨ fittingí•˜ì—¬ word embedding ê°’ì„ êµ¬í•˜ëŠ” ë°©ì‹</li>
      <li>$u_i$: ì…ë ¥ì–´ì˜ ì„ë² ë”© ë²¡í„°, $v_j$: ì¶œë ¥ì–´ì˜ ì„ë² ë”© ë²¡í„°, $P_{ij}$: ìœˆë„ìš° ê¸°ë°˜ ë‘ ë‹¨ì–´ i, jì˜ ë™ì‹œ ë“±ì¥ ë¹ˆë„</li>
    </ul>
  </li>
</ul>

\[J(\theta)=\frac{1}{2}\sum^w_{i,j=1}f(P_{ij})(u_{i}^Tv_j-logP_{ij})^2\]

<p>â€‹		<strong>[math. GloVeì˜ ì†ì‹¤í•¨ìˆ˜]</strong></p>

<ul>
  <li>ìœˆë„ìš° ê¸°ë°˜ ë™ì‹œ ë“±ì¥ ë¹ˆë„($P_{ij}$)ëŠ” ì „ì²´ ë‹¨ì–´ ì§‘í•© ë“¤ì˜ ë‹¨ì–´ë“¤ì´ ìœˆë„ìš° í¬ê¸° ë‚´ì—ì„œ ë‹¨ì–´ê°€ ë“±ì¥í•œ íšŸìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©°, ë³´í†µ ì „ì²´ ë‹¨ì–´ë“¤ì˜ ë“±ì¥ ë¹ˆë„ë¥¼ í–‰ë ¬ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•œë‹¤.
    <ul>
      <li>â€œI like deep learningâ€, â€œI like NLPâ€, â€œI enjoy flyingâ€ ì„¸ ë¬¸ì¥ì´ ì£¼ì–´ì§€ê³  ìœˆë„ìš° í¬ê¸°ê°€ 1ì¼ ë•Œ,</li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: left">ì¹´ìš´íŠ¸</th>
      <th style="text-align: left">I</th>
      <th style="text-align: left">like</th>
      <th style="text-align: left">enjoy</th>
      <th style="text-align: left">deep</th>
      <th style="text-align: left">learning</th>
      <th style="text-align: left">NLP</th>
      <th style="text-align: left">flying</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">I</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">2</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
    </tr>
    <tr>
      <td style="text-align: left">like</td>
      <td style="text-align: left">2</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
    </tr>
    <tr>
      <td style="text-align: left">enjoy</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">1</td>
    </tr>
    <tr>
      <td style="text-align: left">deep</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
    </tr>
    <tr>
      <td style="text-align: left">learning</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
    </tr>
    <tr>
      <td style="text-align: left">NLP</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
    </tr>
    <tr>
      <td style="text-align: left">flying</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">0</td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. ì˜ˆì œì˜ ìœˆë„ìš° ê¸°ë°˜ ë™ì‹œ ë“±ì¥ í–‰ë ¬(Window based Co-occurrence Matrix) https://wikidocs.net/22885]</strong></p>

<ul>
  <li>ì¤‘ë³µ ë˜ëŠ” ê³„ì‚°ì´ ì ì–´ ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¥¼ ìˆ˜ ìˆê³ , ì ì€ ë°ì´í„°ë¡œë„ ì„±ëŠ¥ì´ ì¢‹ë‹¤.(ì‹¤ì œ ì„±ëŠ¥ì€ ë¹„ë“±ë¹„ë“±)</li>
  <li>https://nlp.stanford.edu/projects/glove/ ì˜¤í”ˆì†ŒìŠ¤ glove ëª¨ë¸</li>
</ul>

<h2 id='Recurrent-Neural-Networks-RNNs'>Recurrent Neural Networks(RNNs)</h2>

<h3 id='Basics-of-Recurrent-Neural-Networks-RNNs'>Basics of Recurrent Neural Networks(RNNs)</h3>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216101229026.png" alt="" /></p>

<p><strong>[img. RNNì˜ êµ¬ì¡°, ì¢Œì¸¡ì€ Rolled RNN, ìš°ì¸¡ì€ UnRolled RNNì´ë‹¤.]</strong></p>

<ul>
  <li>
    <p>Sequence ì…ë ¥ ë²¡í„° Xê³¼ ì´ì „ time stepì˜ RNN ëª¨ë“ˆì—ì„œ ê³„ì‚°í•œ $h_{t-1}$ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, í˜„ì¬ì˜ $h_t$ë¥¼ ì¶œë ¥í•˜ëŠ” êµ¬ì¡°.</p>

    <table>
      <thead>
        <tr>
          <th>ìˆ˜ì‹ê³¼ êµ¬ì¡°</th>
          <th>êµ¬ì„±</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210217032720485.png" alt="" /></td>
          <td>\(h_{t-1}:\ ì´ì „\ hidden-state\ vector\\x_t:\ í•´ë‹¹ ëª¨ë“ˆì˜\ input\ vector\\h_t:\ ìƒì„±ëœ\ hidden-sate\ vector\\\cdot ìµœì¢…\ output(y_t)\ ì¶œë ¥ì‹œ\ h_të¥¼\ ë°”íƒ•ìœ¼ë¡œ ê³„ì‚°  \\f_W:\ íŒŒë¼ë¯¸í„°\ Wë¥¼\ í¬í•¨í•œ\ RNN í•¨ìˆ˜\\y_t:\ í•´ë‹¹\ ëª¨ë“ˆì˜\ output\ vecotr\)</td>
        </tr>
      </tbody>
    </table>

    <p><strong>[fig. RNNì—ì„œì˜ hidden state ê³„ì‚°ê³¼ êµ¬ì„± ìš”ì†Œ]</strong></p>
  </li>
  <li>
    <p>ë§¤ time step ë§ˆë‹¤ ê°™ì€ í•¨ìˆ˜ì™€ Parameter êµ¬ì„±ì„ ê³µìœ í•¨</p>
  </li>
</ul>

\[h_t = f_W(h_{t-1},x_t)\\
\downarrow\\
h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)\\
y_t =W_{hy}h_t\]

<p><strong>[math. RNNì˜ ìì„¸í•œ $f_W$í•¨ìˆ˜]</strong></p>

<ul>
  <li>
    <p>Hidden State Vector($h_t$)ì˜ ì°¨ì›ìˆ˜ëŠ” Hyper parameterì´ë‹¤.</p>

    <ul>
      <li>ì¦‰, ê°œë°œì íŒë‹¨í•˜ì— ì£¼ì–´ì ¸ì•¼ í•¨.</li>
    </ul>

    <p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210217040854308.png" alt="" /><br />
<strong>[img. $W$ì˜ ì°¨ì› ìˆ˜ëŠ” $h_{t-1}$ ì°¨ì›ìˆ˜ X ($h_{t-1}$ ì°¨ì›ìˆ˜ + $x_t$ ì°¨ì›ìˆ˜)ê°€ ëœë‹¤.]</strong></p>
  </li>
  <li>
    <p>ë§ˆì§€ë§‰ìœ¼ë¡œ output $y_t$ì˜ ê²½ìš° binary classificationì´ë©´ 1ì°¨ì›ìœ¼ë¡œ ë°”ê¾¼ ë’¤ sigmoid í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©°, Multi Class classificationì´ë©´ nê°œì˜ ì°¨ì›ìœ¼ë¡œ ë°”ê¾¼ ë’¤ SoftMax Layerë¥¼ í†µê³¼ ì‹œí‚¨ë‹¤.</p>
  </li>
</ul>

<h3 id='Types-of-RNNs'>Types of RNNs</h3>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216102655561.png" alt="" /></p>

<p><strong>[img. ì—¬ëŸ¬ ì¢…ë¥˜ì˜ RNN êµ¬ì¡°]</strong></p>

<ul>
  <li>
    <p>one-to-one êµ¬ì¡°</p>

    <ul>
      <li>í‚¤, ëª¸ë¬´ê²Œ, ë‚˜ì´ë¥¼ í†µí•´ ì €í˜ˆì••, ì •ìƒí˜ˆì••, ê³ í˜ˆì••ì„ íŒë‹¨</li>
      <li>Time Stepì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ</li>
    </ul>
  </li>
  <li>
    <p>one-to-many êµ¬ì¡°</p>

    <ul>
      <li>Inputì´ ì²« time stepì— í•œë²ˆ, ì¶œë ¥ì€ ë§¤ë²ˆ,</li>
      <li>ë‚˜ë¨¸ì§€ Inputì€ ë¬´ì˜ë¯¸í•œ ê°™ì€ ì°¨ì›ì˜ zero vectorê°€ ë“¤ì–´ê°„ë‹¤.</li>
      <li>Image Captioning</li>
    </ul>
  </li>
  <li>
    <p>many-to-one êµ¬ì¡°</p>

    <ul>
      <li>
        <p>Inputì€ ë§¤ë²ˆ, Outputì€ ë§ˆì§€ë§‰ í•œë²ˆ</p>
      </li>
      <li>
        <p>ê°ì • ë¶„ì„</p>
      </li>
    </ul>
  </li>
  <li>
    <p>sequence-to-sequence êµ¬ì¡° 1</p>

    <ul>
      <li>Inputì´ ì¼ë¶€ time stepì—, outputì´ ë˜ ì¼ë¶€ time stepì— ì¡´ì¬</li>
      <li>ê¸°ê³„ë²ˆì—­</li>
    </ul>
  </li>
  <li>
    <p>sequence-to-sequence êµ¬ì¡° 2</p>

    <ul>
      <li>Inputê³¼ Outputì´ ë§¤ë²ˆ ì¡´ì¬</li>
      <li>video classification per frame</li>
    </ul>
  </li>
</ul>

<h3 id='Character-level-Language-Model'>Character-level Language Model</h3>

<ul>
  <li>Language Modelì´ë€, ë¬¸ìì—´ì´ë‚˜ ë‹¨ì–´ë¥¼ ì…ë ¥ë°›ê³ , ê·¸ ë‹¤ìŒì— ë‚˜ì˜¬ ë¬¸ìë‚˜ ë‹¨ì–´ë¥¼ ì¶œë ¥í•˜ëŠ” ëª¨ë¸</li>
</ul>

<h4 id='RNN-ì˜ˆì‹œ-hello-Language-Model'>RNN ì˜ˆì‹œ(hello Language Model)</h4>

<ol>
  <li>ì‚¬ì „(Vocabulary) êµ¬ì¶• ë° one-hot vector ìƒì„±</li>
</ol>

<ul>
  <li>ì‚¬ì „(Vocabulary)
    <ul>
      <li>ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ ì •ë³´ ë‹¨ìœ„(ë‹¨ì–´ ë˜ëŠ” ì—¬ê¸°ì„œëŠ” ê¸€ì)ë“¤ì´ ìœ ì¼í•˜ê²Œ ì´ë£¨ì–´ì§„ ì‚¬ì „ì„ êµ¬ì¶•í•œë‹¤.</li>
      <li>ì˜ˆì‹œì˜ â€œhelloâ€ì˜ ê²½ìš° â€œ[h, e, l, o]â€ë¼ëŠ” Vocabularyê°€ êµ¬ì¶•ë¨.</li>
    </ul>
  </li>
  <li>one-hot vector ìƒì„±
    <ul>
      <li>index ë¶€ë¶„ì´ 1ì¸ ì‚¬ì „ì˜ ê°¯ìˆ˜ ë§Œí¼ì˜ dimensionì„ ê°€ì§€ëŠ” one-hot vector ìƒì„±</li>
      <li>ë§ˆì§€ë§‰ â€œoâ€ì˜ ê²½ìš° ê·¸ ë’¤ë¡œ ì˜ˆì¸¡í•´ì•¼í•  í•„ìš” ì—†ìœ¼ë¯€ë¡œ ìƒì„±í•˜ì§€ ì•Šì•„ë„ ì¢‹ë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216104022300.png" alt="" /></p>

<p><strong>[img. helloì˜ one-hot vector]</strong></p>

<ol>
  <li>
    <p>ì„ í˜•ê²°í•©ê³¼ ë¹„ì„ í˜• í•¨ìˆ˜ tanhë¥¼ ì´ìš©í•œ $h_t$ í•™ìŠµ</p>

    <p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216104043006.png" alt="" /></p>

    <p><strong>[img. $h_t$ì˜ dimensionì´ 3ì´ë¼ê³  ê°€ì •í•˜ê³  í•™ìŠµ]</strong></p>

    <ul>
      <li>$h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t+b)$ b: bias</li>
      <li>ìµœì´ˆì˜ RNN ëª¨ë¸ì˜ ê²½ìš° $h_{t-1}$ì…ë ¥ì´ ì—†ìœ¼ë¯€ë¡œ hì™€ ê°™ì€ ì°¨ì›ì˜ zero vectorë¥¼ ì±„ì›Œì¤€ë‹¤.</li>
      <li>ìì„¸í•œ ë°©ë²•ì€ ìœ„ì˜ RNNì˜ êµ¬ì¡° ì°¸ì¡°</li>
    </ul>
  </li>
  <li>
    <p>output ì¶œë ¥</p>
  </li>
</ol>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216104133143.png" alt="" /></p>

<p><strong>[img. output vector ì¶œë ¥]</strong></p>

<ul>
  <li>ê° RNN ëª¨ë“ˆì˜ outputì€ hidden state($h_t$)ì— $W_{hy}$ë¥¼ ê³±í•œ í›„ biasë¥¼ ë”í•´ì„œ êµ¬í•˜ê²Œ ëœë‹¤.
    <ul>
      <li>$Logit = W_{hy}h_t+b$</li>
    </ul>
  </li>
  <li>ì´í›„ Outputì„ softmax í•¨ìˆ˜ì— í†µê³¼ ì‹œì¼œ ê°€ì¥ í° ê°’ì„ ê²°ê³¼ê°’ìœ¼ë¡œ í™•ì •í•œë‹¤.</li>
  <li>ì´í›„ ground-truth(ì‹¤ì œ ê°’)ì˜ one-hot vectorì™€ ë¹„êµí•˜ì—¬ lossë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ë‹¤.</li>
</ul>

<ol>
  <li>
    <p>í•™ìŠµì´ ëë‚œ ë’¤ Test inference ì‹œí–‰</p>

    <ul>
      <li>
        <p>ìµœì´ˆì˜ Time stepì—ë§Œ ì…ë ¥ì„ ë„£ì–´ì£¼ê³ , ì´í›„ Time step ë¶€í„°ëŠ” ì´ì „ ëª¨ë“ˆì˜ outputì„ Inputìœ¼ë¡œ ë„£ì–´ì£¼ì–´ ì¶”ë¡ í•œë‹¤.</p>
      </li>
      <li>
        <p>ë‹¤ìŒë‚  ì£¼ì‹ì„ ì˜ˆì¸¡í•˜ëŠ” RNN ëª¨ë¸ì´ ê·¸ ë‹¤ìŒë‚ , ë‹¤ìŒ ë‹¤ìŒë‚ ì—ë„ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì´ìœ </p>
      </li>
    </ul>
  </li>
</ol>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216104612495.png" alt="" /></p>

<p><strong>[img. ì¶œë ¥ì„ ë‹¤ìŒ ëª¨ë“ˆì˜ inputìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” Inference êµ¬ì¡°]</strong></p>

<h4 id='RNN-ì˜ˆì œ'>RNN ì˜ˆì œ</h4>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210217105519872.png" alt="" /></p>

<p><strong>[img. ì…°ìµìŠ¤í”¼ì–´ í¬ê³¡ ìƒì„±]</strong></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210217105602228.png" alt="" /></p>

<p><strong>[img. ì˜í™” ëŒ€ë³¸ ìƒì„±]</strong></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210217105630027.png" alt="" /></p>

<p><strong>[img. Cì–¸ì–´ code ìƒì„±]</strong></p>

<h4 id='Backpropagation-through-time-BPTT'>Backpropagation through time (BPTT)</h4>

<ul>
  <li>ëª¨ë“  RNN Time stepì— ëŒ€í•´ Backpropagtion ë˜ëŠ” ForwardPropagtionì„ ì§„í–‰í•˜ëŠ” ê²ƒì€ ì„±ëŠ¥ìƒ í•œê³„ê°€ ìˆë‹¤.</li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216110815279.png" alt="" /></p>

<p><strong>[img. RNNì—ì„œì˜ Forwardì™€ backpropagation]</strong></p>

<ul>
  <li>Truncation : ì „ì²´ë¥¼ propagation í•˜ë©´ ë„ˆë¬´ ëŠë¦¬ë¯€ë¡œ êµ¬ê°„ ë³„(chunks of the sequence)ë¡œ ì˜ë¼ì„œ í•œë‹¤.</li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216110934728.png" alt="" /></p>

<p><strong>[img. Truncation  : êµ¬ê°„ë³„ë¡œ ì˜ë¼ì„œ propagation]</strong></p>

<ul>
  <li>Hidden state Vector($h_t$)ëŠ”  ì´ì „ ì²˜ë¦¬ ê²°ê³¼, ë¬¸ë§¥ ë“±ì˜ í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤.
    <ul>
      <li>ì´ëŸ¬í•œ Hidden State Vectorì˜ ë³€í™”ë¥¼ ë¶„ì„í•˜ëŠ” ê²ƒìœ¼ë¡œ RNNì˜ íŠ¹ì„±ì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210217115424822.png" alt="" /></p>

<p><strong>[img. Hidden stateì˜ ë³€í™”ë¥¼ ì‹œê°í™”í•œ ê²ƒ.]</strong></p>

<h4 id='Vanishing-Exploding-Gradient-Problem-in-RNN'>Vanishing/Exploding Gradient Problem in RNN</h4>

<ul>
  <li>RNNì—ëŠ” Long-term problemê³¼ Vanishing/Exploding Gradient Problemì„ ê°€ì§€ê³  ìˆë‹¤.
    <ul>
      <li>ê°€ì¤‘ì¹˜ê°€ ê³„ì† ê³±í•´ì ¸ì„œ Gradientê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ë¬¸ì œ</li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>math</th>
      <th>propagation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$h_t = tanh(w_{xh}x_t+w_{hh}h_{t-1}+b), t=1,2,3\For\ w_{hh}=3,w_{xh}=2,b=1\h_3=tanh(2x_3+3h_2+1)\h_2=tanh(2x_2+3h_1+1)\h_1=tanh(2x_1+3h_0+1)\\dots\h3=tanh(2x_3+3tanh(2x_2+3h_1+1)+1)$</td>
      <td><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216111919764.png" alt="" /></td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. hidden state ì¤‘ì²©ì— ë”°ë¥¸ $W_{hh}$ì˜ ì¤‘ì²© ]</strong></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/vanishing gradient.gif" alt="" /></p>

<p><strong>[gif. LSTMê³¼ RNNì˜ cell ì§„í–‰ì— ë”°ë¥¸ gradient ê°ì†Œ ë¹„êµ]</strong></p>

<h3 id='Long-Short-Term-Memory-LSTM-and-Gated-Recurrent-Unit-GRU'>Long Short-Term Memory(LSTM) and Gated Recurrent Unit(GRU)</h3>

<ul>
  <li>ê¸°ì¡´ì˜ RNN ëª¨ë¸ì— ë¹„í•´ Vanishing/Exploding gradient ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ì„±ëŠ¥ ìƒì— ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤.</li>
</ul>

<h4 id='Long-Short-Term-Memory-LSTM'>Long Short-Term Memory(LSTM)</h4>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216113818583.png" alt="" />\</p>

<p><strong>[img. LSTM êµ¬ì¡°ì˜ ë„ì‹í™”]</strong></p>

<ul>
  <li>ì •ë³´ë¥¼ ì¢€ ë” ì˜¤ë˜ ë‚¨ê¸°ê²Œ í•˜ê³  ê¸°ì¡´ì˜ RNN ëª¨ë¸ì„ ê°œì„ í•œ ëª¨ë¸.</li>
  <li>Vanishing/Exploding  Gradient Problem, Long-Term dependency ë¬¸ì œë¥¼ í•´ê²°í•œ ëª¨ë¸</li>
  <li>Hidden state vector($h_t$)ë¥¼ ê¸°ì–µ ì†Œìë¡œ ë³´ê³ , ë‹¨ê¸° ê¸°ì–µì„ ê¸¸ê²Œ ê¸°ì–µí•˜ë„ë¡ ê°œì„ í•˜ì—¬ LSTMì´ë¼ê³  ì´ë¦„ ë¶™ì„.</li>
  <li>ì´ì „ Time stepì—ì„œ ë„˜ì–´ì˜¤ëŠ” ì •ë³´(Cell state($C_t$),  Hidden state($h_{t-1}$) )ê°€ 2ê°œì´ë©°, ì´ Inputì€ 3ê°œì´ë‹¤.
    <ul>
      <li>${C_t, h_t}= LSTM(x_t,C_{t-1},h_{t-1})$</li>
      <li>$C_t$ : Cell state vector, í•µì‹¬ ì •ë³´</li>
      <li>$h_{t-1}$: Hidden state vector, Cell state ì •ë³´ë¥¼ í•„ìš”í•œ ê²ƒë§Œ Filtering í•œ ì •ë³´.</li>
    </ul>
  </li>
</ul>

<h4 id='Long-short-Term-Memoryì˜-êµ¬ì„±ê³¼-ë™ì‘-ê³¼ì •'>Long short Term Memoryì˜ êµ¬ì„±ê³¼ ë™ì‘ ê³¼ì •</h4>

<ul>
  <li>ì…ë ¥ì„ ë°›ì€ í›„ ì„ í˜• ë³€í™˜ í›„ ë‚˜ì˜¨ ê²°ê³¼ë¬¼ì— 4ê°œì˜ activation í•¨ìˆ˜ë¥¼ í†µê³¼ ì‹œì¼œ gateê°’ë“¤ì„ ë§Œë“ ë‹¤.</li>
  <li>ì´ë ‡ê²Œ ë‚˜ì˜¨ gate ë“¤ì€ Cell state ë° Hidden Stateë¥¼ ê³„ì‚°í•  ë•Œì˜ ì¤‘ê°„ ê²°ê³¼ë¬¼ ì—­í• ì„ í•œë‹¤.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>LSTM êµ¬ì„±</th>
      <th>ìˆ˜ì‹</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216115034645.png" alt="" /></td>
      <td>$\begin{pmatrix}i \f\o\g \end{pmatrix}=\begin{pmatrix}\sigma \\sigma\\sigma\tanh \end{pmatrix}W\begin{pmatrix}h_{t-1}\x_t\end{pmatrix}\c_t=f\odot c_{t-1}+i\odot g\h_t=o\odot tanh(c_t)$</td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. LSTM ë‚´ë¶€ì˜ ì—°ì‚°, $x_t, h_t$ì˜ dimensionì„ hë¼ê³  ê°€ì •]</strong></p>

<ul>
  <li>i: Input gate, Whether to write to cell</li>
  <li>f: Forget gate, Whether to erase cell</li>
  <li>o: Output gate, How much to reveal cell
    <ul>
      <li>sigmoidë¥¼ í†µí•´ ë‚˜ì˜¤ëŠ” Input gate, Forget gate, Output gateëŠ” 0~1ì‚¬ì´ë¥¼ ê°€ì§€ë©° ë‹¤ë¥¸ ë²¡í„°ì™€ ê³±í•´ì ¸ 0~1 ì‚¬ì´ì˜ ì¼ë¶€ë¡œ ì¶•ì†Œí•´ì£¼ëŠ” ì—­í• ì„ í•¨(gate)</li>
    </ul>
  </li>
  <li>g: Gate gate, How much to write to cell</li>
</ul>

<h4 id='Gate-ë“¤ì˜-êµ¬ì²´ì ì¸-ì˜ˆì‹œ'>Gate ë“¤ì˜ êµ¬ì²´ì ì¸ ì˜ˆì‹œ</h4>

<ul>
  <li>gateë“¤ì€ ì´ì „ì˜ $c_{t-1}$ì„ ì ì ˆí•˜ê²Œ ê°€ê³µí•˜ëŠ” ì—­í• ì„ í•¨</li>
</ul>

<ol>
  <li>Forget gate</li>
</ol>

<table>
  <thead>
    <tr>
      <th>ìˆ˜ì‹</th>
      <th>ë„ì‹</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$f_t=\sigma(W_f \cdot [h_{t-1},x_t]+b_f)$</td>
      <td><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216135905387.png" alt="" /></td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. forget gate ìˆ˜ì‹ê³¼ ê·¸ë¦¼]</strong></p>

<ul>
  <li>forget gateëŠ” sigmoid í•¨ìˆ˜ë¥¼ í†µí•´ $h_{t-1}$ê³¼ $x_t$ì˜ ì¼ë¶€ ë°ì´í„°ë¥¼ ì¶•ì†Œí•˜ëŠ” ì—­í• </li>
</ul>

<ol>
  <li>Gate gate &amp; Input gate</li>
</ol>

<table>
  <thead>
    <tr>
      <th>ìˆ˜ì‹</th>
      <th>ë„ì‹</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$i_t=\sigma(W_i\cdot [h_{t-1},x_t]+bi)\ (input\ gate) \\widetilde{C_t}=tanh(W_c\cdot [h_{t-1},x_t]+b_C)\  (gate\ gate)\C_t=f_t\cdot C_{t-1}+i_t\cdot \widetilde{C_t}\  (input\ gate \times gate\ gate)$</td>
      <td><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216135817334.png" alt="" /></td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. Input gateì™€ Gate gateì˜ ìˆ˜ì‹ê³¼ ê·¸ë¦¼]</strong></p>

<ul>
  <li>Input gateì™€ Gate gateì˜ ê²°ê³¼ë¬¼ì„ ê³±í•œ ë’¤, í•´ë‹¹ ê²°ê³¼ë¬¼ì„ Forget gateì™€ ì´ì „ Cell stateë¥¼ ê³±í•œ ê²ƒì„ ë”í•˜ì—¬ í˜„ì¬ì˜ Cell stateë¥¼ ë§Œë“¤ê²Œ ëœë‹¤.
    <ul>
      <li>2ë²ˆì˜ ì„ í˜•ë³€í™˜ë¥¼ ê±°ì¹œ í˜„ì¬ ì…ë ¥ì •ë³´($h_{t-1}, x_t$)ë¥¼ ì´ì „ Cell stateì™€ í•©ì³ ìƒˆë¡œìš´ stateë¥¼ ë§Œë“œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.</li>
    </ul>
  </li>
</ul>

<ol>
  <li>Output gate</li>
</ol>

<table>
  <thead>
    <tr>
      <th>ìˆ˜ì‹</th>
      <th>ë„ì‹</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$o_t=\sigma(W_o[h_{t-1},x_t]+b_o) \ h_t=o_t\cdot tanh(C_t)$</td>
      <td><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216135945524.png" alt="" /></td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. Output gateì˜ ìˆ˜ì‹ê³¼ ê·¸ë¦¼]</strong></p>

<ul>
  <li>ìƒì„±í•œ Output gateì˜ ê²°ê³¼ë¥¼ tanhí•¨ìˆ˜ë¥¼ í†µê³¼ì‹œì¼œ, í˜„ì¬ í•„ìš”í•œ ì •ë³´ë§Œ filteringí•œ ë’¤, í˜„ì¬ Cell stateì™€ ê³±í•˜ì—¬  Hidden stateë¥¼ ë§Œë“ ë‹¤.</li>
  <li>tanh í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•  ì‹œ ë¯¸ë¶„ê°’ì´ sigmoidì— ë¹„í•´ ì»¤ì„œ vanishing/exploding gradient problemì„ í•´ê²°í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<h4 id='Gated-Recurrent-Unit-GRU'>Gated Recurrent Unit(GRU)</h4>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216133902631.png" alt="" /></p>

<p><strong>[img. GRU êµ¬ì¡°]</strong></p>

<ul>
  <li>LSTMì„ ê°„ì†Œí™”í•˜ì—¬ ë”ìš± ì ì€ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ê³¼ ë¹ ë¥¸ ê³„ì‚° ì‹œê°„ì„ ê°€ëŠ¥í•˜ê²Œ í•œ ëª¨ë¸</li>
  <li>LSTMê³¼ ë‹¬ë¦¬ Cell stateê°€ ì¡´ì¬ í•˜ì§€ ì•Šê³ , hidden state ë²¡í„° í•˜ë‚˜ë§Œ ë‹¤ìŒ Time stepìœ¼ë¡œ ë³´ë‚¸ë‹¤.</li>
  <li>
    <p>LSTMê³¼ ë”ë¶ˆì–´ ë§ì´ ì‚¬ìš©ë˜ë©° ì„±ëŠ¥ìƒ ë’¤ì§€ì§€ ì•Šìœ¼ë©´ì„œ ë¹ ë¥´ê²Œ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤.</p>
  </li>
  <li>GRUì˜ $h_{t-1}$ì´ LSTMì˜ Cell state ì´ì „ time step ë“¤ì˜ ì •ë³´ì™€ ê²°ê³¼ê°’ì„ í•¨ê»˜ í¬í•¨í•œë‹¤.
    <ul>
      <li>$z_t=\sigma(W_z\cdot [h_{t-1},x_t])$ (Input gate)</li>
      <li>$r_t=\sigma(W_r\cdot [h_{t-1},x_t])$</li>
      <li>$\widetilde{h_t}=tanh(W\cdot[r_t\cdot h_{t-1},x_t])$ (LSTMì˜ Gate gate ì—­í• )</li>
      <li>$h_t=(1-z_t)\cdot h_{t-1}+z_t\cdot \widetilde{h_t}$</li>
      <li>c.f) $C_t=f_t\cdot C_{t-1}+i_t\cdot \widetilde{C_t}$ in LSTM,
        <ul>
          <li>Input gateì™€ Forget gateì˜ ëŒ€ì‹ ì¸ 1 - Input gateí•œ ê°’ìœ¼ë¡œ hidden stateë¥¼ êµ¬í•œë‹¤.(ê°€ì¤‘ í‰ê· ì˜ í˜•íƒœ)</li>
          <li>ë‚´ë¶€ì ìœ¼ë¡œ 1ê°œì˜ gateë¡œ í†µí•©í•˜ë©´ì„œ ê³„ì‚°ëŸ‰ì„ ì¤„ì„</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id='Backpropagation-in-GRU'>Backpropagation in GRU</h5>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210216134033070.png" alt="" /></p>

<p><strong>[img. GRU propagation]</strong></p>

<ul>
  <li>ê³±ì…ˆì´ ì•„ë‹ˆë¼ ë§ì…ˆìœ¼ë¡œ ì—°ì‚°í•´ì„œ Vanishing/Exploding gradient problemì„ í•´ê²°í•¨
    <ul>
      <li>$h_t=(1-z_t)\cdot h_{t-1}+z_t\cdot \widetilde{h_t}$</li>
    </ul>
  </li>
  <li>Long term dependency ë¬¸ì œ í•´ê²°</li>
</ul>

<h4 id='Summary-on-RNN-LSTM-GRU'>Summary on RNN/LSTM/GRU</h4>

<ul>
  <li>RNNì€ ë‹¤ì–‘í•œ ê¸¸ì´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆëŠ” Sequence dataì— íŠ¹í™”ëœ ìœ ì—°í•œ í˜•íƒœì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¡°</li>
  <li>Vanilla RNNì€ ê°„ë‹¨í•œ êµ¬ì¡°ì§€ë§Œ í•™ìŠµì‹œ ë¬¸ì œê°€ ë§ê³  í•™ìŠµì´ ì˜ ì•ˆëœë‹¤.</li>
  <li>LSTMê³¼ GRUëŠ” long term problemê³¼ vanishing\exploding gradient ë¬¸ì œë¥¼ í•´ê²°í–ˆìŒ</li>
  <li>ì¤‘ì²©ë˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ê³±ì…ˆì´ ì•„ë‹Œ ë§ì…ˆìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë¬¸ì œ í•´ê²°</li>
</ul>

<h2 id='Sequence-to-Sequence-with-Attention'>Sequence to Sequence with Attention</h2>

<h3 id='Seq2Seq-amp-Encoder-decoder-amp-attention'>Seq2Seq &amp; Encoder-decoder &amp; attention</h3>

<h4 id='Sequence-to-Sequence-amp-Encoder-decode'>Sequence to Sequence &amp; Encoder-decode</h4>

<p>Sequence to SequenceëŠ” Sequence dataë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì€ í›„ Sequence dataë¥¼ ì¶œë ¥í•˜ëŠ” many to many ëª¨ë¸ì„ ë§í•œë‹¤.</p>

<p>ìì—°ì–´ ì²˜ë¦¬, ê¸°ê³„ ë²ˆì—­ ë“±ì´ ì´ì— í•´ë‹¹í•œë‹¤.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210218082530068.png" alt="" /></p>

<p><strong>[img. ì±—ë´‡ì˜ LSTM Encoder Decoder ì˜ˆì‹œ ]</strong></p>

<p>Encoder-Decoder ëª¨ë¸ì´ ëŒ€í‘œì ì´ë©°, EncoderëŠ” ì…ë ¥ ë¬¸ì¥ì„ ë°›ì•„ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸ì´ë©°, DecoderëŠ” ì¶œë ¥ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ë©°, ì„œë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤.</p>

<p>Encoderì˜ ì…ë ¥ ì²˜ë¦¬ ì •ë³´ê°€ ë‹´ê¸´ ë§ˆì§€ë§‰ $h_t$(hidden state)ëŠ” Decoder ëª¨ë¸ì˜ ì²«ë²ˆì§¸ Input $h_t$(hidden state) vectorê°€ ëœë‹¤.</p>

<p>DecoderëŠ” ì²«ë²ˆì§¸ Inputìœ¼ë¡œ Start Token(ë¯¸ë¦¬ ì •ì˜ëœ íŠ¹ìˆ˜ ë¬¸ì, <SoS>)ê³¼ ë§ˆì§€ë§‰ hidden stateì„ ì…ë ¥ ë°›ì€ ë’¤, ìˆœì°¨ì ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê³  í•´ë‹¹ ê²°ê³¼ë“¤Output Token, $h_t$)ì€ ë‹¤ìŒ Time stepì˜ Inputì´ ëœë‹¤.</SoS></p>

<p>Decoderì˜ ì¶œë ¥ì´ ëë‚˜ë©´ End Token(ë¯¸ë¦¬ ì •ì˜ëœ íŠ¹ìˆ˜ ë¬¸ì2, <EoS>)ì„ ë‚´ë³´ë‚´ê²Œ ëœë‹¤.</EoS></p>

<h4 id='Attention'>Attention</h4>

<p>ê¸°ì¡´ì˜ Seq2Seq modelì˜ ë‹¨ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<ul>
  <li>Hidden state($h_t$)ì˜ ì°¨ì›ì´ ê³ ì •ë˜ì–´ ìˆì–´, ì—¬ëŸ¬ ì •ë³´ë¥¼ ìš°ê²¨ë„£ì–´ì•¼ í•˜ëŠ” ì </li>
  <li>Time stepì´ ì§€ë‚˜ê°ˆ ìˆ˜ë¡ ë‚´í¬í•˜ê³  ìˆëŠ” ì •ë³´ê°€ ì†Œì‹¤ ë˜ê±°ë‚˜ ìœ ì‹¤ë˜ëŠ” ì </li>
</ul>

<p>ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Input Sequenceë¥¼ ê±°ê¾¸ë¡œ ì£¼ëŠ” ë°©ë²• ë“±ì´ ì œì‹œë˜ì—ˆë‹¤.</p>

<p>ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Attention ëª¨ë“ˆì˜ ì•„ì´ë””ì–´ëŠ”</p>

<p>â€‹	ë§ˆì§€ë§‰ Hidden state($h_t$) í•˜ë‚˜ë§Œ ì „ë‹¬í•´ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë§¤ Time stepì—ì„œ ê³„ì‚°í•œ $h_0\dots h_t$ê¹Œì§€ ì „ì²´ì ìœ¼ë¡œ Decoderì— ì „í•´ì¤€ ë’¤, Decoderì˜ ê° Time stepì—ì„œ ì„ ë³„ì ìœ¼ë¡œ í•„ìš”í•œ Hidden stateë¥¼ ê°€ì ¸ê°„ë‹¤.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/nmt-model-fast.gif" alt="" /></p>

<p><strong>[img. Attention ëª¨ë“ˆ ë™ì‘ ê³¼ì •]</strong></p>

<h5 id='ìì„¸í•œ-Attention-ëª¨ë“ˆ-ë™ì‘-ê³¼ì •'>ìì„¸í•œ Attention ëª¨ë“ˆ ë™ì‘ ê³¼ì •</h5>

<p>ê¸°ê³„ë²ˆì—­ ì˜ˆì‹œë¥¼ í†µí•´ Attention ëª¨ë“ˆ ë™ì‘ ê³¼ì •ì„ ì•Œì•„ë³´ì.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210218110702268.png" alt="" /></p>

<p><strong>[img. Attention ëª¨ë“ˆ ì˜ˆì‹œ 1~5 ]</strong></p>

<ol>
  <li>Encoder ëª¨ë“ˆì´ ëª¨ë“  ê³¼ì •ì„ ëë‚´ê³  ë§ˆì§€ë§‰ Hidden stateì™€ Start Tokenì„ Decoderì˜ ì²«ë²ˆì§¸ ëª¨ë“ˆì´ Inputìœ¼ë¡œ ë°›ëŠ”ë‹¤, ì´ë•Œ, Encoder ê° Time stepì—ì„œì˜ hidden state($h_0,\dots,h_{t-1}$)ë“¤ ë˜í•œ ê¸°ì–µë˜ì–´ ìˆë‹¤.</li>
  <li>ì²«ë²ˆì§¸ Decoder Hidden stateì˜ ê²°ê³¼ê°’ $h_{y0}$ë¥¼ Encoderì˜ ê° Time stepì˜ Hidden stateë“¤ê³¼ ë‚´ì í•˜ì—¬ Attention scoresë¥¼ êµ¬í•œë‹¤.
    <ul>
      <li>ì˜ˆì‹œë¥¼ ë“¤ìë©´ ë¬´ì‘ìœ„ì˜ ì •ìˆ˜ ê°’ì´ ë‚˜ì˜¨ë‹¤.{7,1,-1,2}</li>
      <li>ì´ë•Œ Attention scoresë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì€ ë‚´ì ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ë°©ë²•ë„ ìˆë‹¤. ì•„ë˜ì˜ Attention Mechanism ì°¸ì¡°</li>
    </ul>
  </li>
  <li>í•´ë‹¹ Attention Scoresë¥¼ softmax í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œì¼œ Attention distributionìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤.
    <ul>
      <li>í•©ì´ 1ì¸ ê°™ì€ ì°¨ì›ì˜ ê²°ê³¼ê°’ì´ ë‚˜ì˜¨ë‹¤. {0.83,0.05,0.02,0.1}</li>
      <li>ì´ ê°’ì€ ì¼ì¢…ì˜ ì–´ë–¤ ê°€ì¤‘ì¹˜ì´ë©°, ì–´ëŠ ì‹œì ì˜ hidden stateë¥¼ ì–¼ë§Œí¼ ì°¸ì¡°í•´ì•¼í•˜ëŠ”ê°€? ì˜ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.</li>
    </ul>
  </li>
  <li>Attention distributionì˜ ê°€ì¤‘ í‰ê· ì„ êµ¬í•˜ì—¬ í•˜ë‚˜ì˜ Encoder Hidden state vector, Attention outputì„ êµ¬í•œë‹¤.
    <ul>
      <li>Attention outputì€ Attention distribution ë§Œí¼ ì •ë³´ë¥¼ ì‚¬ìš©í•´ ë§Œë“  ì •ë³´ì´ë©°, Context vectorë¼ê³ ë„ ë¶€ë¥¸ë‹¤.</li>
      <li>2ë²ˆë¶€í„° 4ë²ˆê¹Œì§€ ê³¼ì •ì„ ì‹¤ì‹œí•˜ëŠ” ë¶€ë¶„ì„ Attention moduleì´ë¼ ë¶€ë¥¸ë‹¤.</li>
    </ul>
  </li>
  <li>ì´ë ‡ê²Œ ë‚˜ì˜¨ Attention Outputê³¼ ì›ë³¸ Outputë¥¼ Concatenationí•˜ì—¬ ì˜ˆì¸¡ ê°’($\hat{y_1}$)ì„ ì¶œë ¥í•œë‹¤.</li>
</ol>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210218110721135.png" alt="" /></p>

<p><strong>[img. Inputì„ ì´ì „ ì˜ˆì¸¡ê°’ì´ ì•„ë‹ˆë¼ ì‹¤ì œê°’ì—ì„œ ê°€ì ¸ì˜¤ë©´ Teacher forcingì´ë‹¤. ]</strong></p>

<ol>
  <li>1ë²ˆë¶€í„° 5ë²ˆê¹Œì§€ì˜ ê³¼ì •ì„ ë‹¤ìŒ Time stepì—ì„œë„ ë°˜ë³µí•œë‹¤.
    <ul>
      <li>ì´ë•Œ, ì´ì „ ì˜ˆì¸¡ ê²°ê³¼ê°’ì„ Inputìœ¼ë¡œ ë„£ì–´ì£¼ì§€ ì•Šê³ , Ground Truthì—ì„œ ê°€ì ¸ì˜¨ ì‹¤ì œê°’ì„ ë„£ì–´ì£¼ëŠ” ì‚¬ì§„ì—ì„œì˜ ë°©ì‹ì„ Teacher forcingì´ë¼ê³  í•œë‹¤.</li>
      <li>ì²« ì˜ˆì¸¡ë¶€í„° í‹€ë¦¬ê³ , í‹€ë¦° ì˜ˆì¸¡ ê°’ì„ ë‹¤ìŒ Inputìœ¼ë¡œ ë„£ì–´ì£¼ë©´ ë¬´ë”ê¸°ë¡œ í‹€ë¦¬ë©´ì„œ ì‹œê°„ë‚­ë¹„ë¥¼ í•˜ê¸° ë•Œë¬¸ì— ë³´í†µì€ ì´ˆë°˜ í•™ìŠµì—ëŠ” Teacher forcingì„ ì´ìš©í•˜ê³ , ì˜ˆì¸¡ ì •í™•ë„ê°€ ì˜¬ë¼ê°€ë©´ ì´ì „ ì˜ˆì¸¡ê°’ì„ ë„£ì–´ì£¼ëŠ” ì›ë˜ ë°©ì‹ìœ¼ë¡œ ëŒë¦°ë‹¤.</li>
    </ul>
  </li>
</ol>

<h5 id='ë‹¤ì–‘í•œ-Attention-ë©”ì»¤ë‹ˆì¦˜'>ë‹¤ì–‘í•œ Attention ë©”ì»¤ë‹ˆì¦˜</h5>

\[score(h_t,\overline{h}_s)=\begin{cases}h_t^\top \overline{h}_s&amp;dot\\h_t^\top W_a\overline{h}_s&amp;general\\v_a^\top \tanh(W_a[h_t;\overline{h}_s]) &amp; concat\end{cases}\\\overline{h}_s : Encoderì—ì„œì˜\ Hidden\ state,\\ h_t^\top: Decoder\ Hidden\ stateê°€\ í–‰ë ¬\ ì—°ì‚°ì„\ ìœ„í•´\ í–‰ê³¼\ ì—´ì´\ ë’¤ë°”ë€\ í˜•íƒœ\\ (a \times b \rightarrow b \times a)\]

<p><strong>[math. attentionì˜ scoreë¥¼ êµ¬í•˜ëŠ” 3ê°€ì§€ ë°©ë²• ]</strong></p>

<ul>
  <li>Dot ($h_t^\top \overline{h}_s$)
    <ul>
      <li>ê¸°ë³¸ì ì¸ ë°©ì‹, ë‘ Hidden stateë¥¼ ë‚´ì í•œë‹¤.</li>
    </ul>
  </li>
  <li>General($h_t^\top W_a\overline{h}_s$)
    <ul>
      <li>í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° $W_a$ë¥¼ ë„£ì–´ score ê³„ì‚° ë°©ë²• ë˜í•œ Back propagationì„ í†µí•´ í•™ìŠµë˜ê²Œ í•œë‹¤.</li>
    </ul>
  </li>
  <li>Concat($v_a^\top \tanh(W_a[h_t;\overline{h}_s])$, Bahdanau attention)
    <ul>
      <li>ë‘ ë²¡í„°ë¥¼ Concatenationí•˜ì—¬ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë§Œë“¤ê³ , $W_a$ì™€ tanh í•¨ìˆ˜, Scalar ê°’ìœ¼ë¡œ ë°”ê¿”ì¤„ ë²¡í„° $v_a^\top$ì„ í†µí•˜ì—¬ ì‘ì€ Multi Layer Networkë¥¼ ë§Œë“¤ì–´ í•™ìŠµë˜ê²Œ í•œë‹¤.</li>
    </ul>
  </li>
</ul>

<p>Generalì´ë‚˜ Concat ë°©ì‹ ì²˜ëŸ¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ í¬í•¨ë˜ë©´ ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì€ Propagationì„ ì§„í–‰ë˜ê²Œ ëœë‹¤.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210218125206543.png" alt="" /></p>

<p><strong>[img. Attention ëª¨ë“ˆì´ í¬í•¨ëœ modelì˜ propagation ì§„í–‰]</strong></p>

<h5 id='Attentionì˜-ì¥ì '>Attentionì˜ ì¥ì </h5>

<ol>
  <li>hiddenê¸°ê³„ ë²ˆì—­ ë¶„ì•¼(NMT)ì—ì„œ ì„±ëŠ¥ í–¥ìƒ</li>
  <li>bottleneck(ë³‘ëª©) ë¬¸ì œ í•´ê²°
    <ul>
      <li>Bottleneck í˜„ìƒ: ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°ì— ë„ˆë¬´ ë§ì€ ë²¡í„° ì •ë³´ë¥¼ ì••ì¶•ì‹œí‚¤ë©´ì„œ ì •ë³´ê°€ ì†ì‹¤, ë³€í˜• ë˜ë©° ì„±ëŠ¥ì´ ì•…í™”ë˜ëŠ” í˜„ìƒ</li>
    </ul>
  </li>
  <li>Vanishing gradient Problem ì™„í™”
    <ul>
      <li>Back Propagation ì‹œ, ë¨¼ ê±°ë¦¬ë¥¼ ê±°ì¹˜ì§€ ì•Šê³ , íŠ¹ì • Time stepì— ë„ë‹¬í•˜ë¯€ë¡œ ê°€ì¤‘ì¹˜ ì¤‘ì²©ì´ ì ë‹¤.</li>
    </ul>
  </li>
  <li>ê°€ì¤‘ì¹˜ì˜ ë³€ê²½ í•´ì„ì„ í†µí•´ ì˜ˆì¸¡ë°©ë²•ì„ í•´ì„í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ.</li>
</ol>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210218125835278.png" alt="" /></p>

<p><strong>[img. xì¶• í”„ë‘ìŠ¤ ë‹¨ì–´, yì¶• í•´ì„í•œ ì˜ë‹¨ì–´ë¥¼ í† ëŒ€ë¡œ ì–¼ë§ˆë‚˜ ì •ë³´ë¥¼ ì°¸ì¡°í–ˆëŠ” ê°€ë¥¼ ê·¸ë˜í”„ë¡œ í‘œí˜„ ê°€ëŠ¥]</strong></p>

<h3 id='Beam-search'>Beam search</h3>

<ul>
  <li>ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ Seq2Seq ëª¨ë¸ì˜ Testì—ì„œ ë”ìš± ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜ì˜¤ê²Œ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜</li>
</ul>

<h4 id='Decoding-ê²°ê³¼ê°’-ì˜ˆì¸¡-ë°©ë²•-ë°-ë¬¸ì œì '>Decoding ê²°ê³¼ê°’ ì˜ˆì¸¡ ë°©ë²• ë° ë¬¸ì œì </h4>

<p>Decoderê°€ ì˜ˆì¸¡ê°’ì„ ìƒì„±í•  ë•Œ, 3ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.</p>

<ol>
  <li>Greedy decoding(íƒìš• ì•Œê³ ë¦¬ì¦˜)</li>
</ol>

<p>ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ì˜ˆì¸¡ì„ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë°”ë¡œ ë‚´ë³´ë‚´ëŠ” ë°©ë²•ì´ë‹¤.</p>

<p>ê°€ì¥ ë¹ ë¥´ë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, í•œë²ˆ ì˜ˆì¸¡ì´ í‹€ë¦¬ë©´ ê·¸ ë’¤ë¡œ ì •ì •í•  ë°©ë²•ì´ ì—†ë‹¤.</p>

<p>ì¦‰, ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ ë’¤ë¡œëŠ” ê°€ì¥ ë‚®ì€ í™•ë¥ ì¸ í‹€ë¦° ë‹¨ì–´ ë“¤ë§Œ ì¡´ì¬í•  ê²½ìš°ë¥¼ ë°©ì§€í•  ìˆ˜ ì—†ë‹¤.</p>

<ol>
  <li>Exhaustive search(ì™„ì „ íƒìƒ‰)</li>
</ol>

<p>ë ˆí¼ëŸ°ìŠ¤ ê¸¸ì´ê°€ Tì¸ ë¬¸ì¥ì— ë‹¨ì–´ë“¤ì˜ ì •ë‹µ í™•ë¥ ì„ $y_0,\dots,y_{t-1}$ì´ë¼ í•  ë•Œ, <br />
\(P(y|x)=P(y_1|x)P(y_2|y_1,x)P(y_3|y_2,y_1,x)\dots P(y_T|y_1,\dots,y_{T-1},x)=\prod^T_1P(y_t|y_1,\dots,y_{t-1},x)\)</p>

<p><strong>[math. ë ˆí¼ëŸ°ìŠ¤ ê¸¸ì´ê°€ Tì¸ ë¬¸ì¥ì˜ ì •ë‹µ í™•ë¥   ]</strong></p>

<table>
  <tbody>
    <tr>
      <td>$P(y</td>
      <td>x)$ê°€ ê°€ì¥ ë†’ì€ ê°’ì´ ë˜ë„ë¡ í•˜ê¸°ìœ„í•´, ê°€ëŠ¥í•œ ëª¨ë“  ë‹¨ì–´ìŒì„ í™•ì¸í•´ë³´ëŠ” ë°©ë²•ì´ ìˆë‹¤.</td>
    </tr>
  </tbody>
</table>

<p>í•˜ì§€ë§Œ ì´ ë°©ë²•ì˜ ê²½ìš° ì‚¬ì „(Vocabulary size)ê°€ Vì´ê³ , ë¬¸ì¥ì˜ ê¸¸ì´ê°€ të¼ê³  í• ë•Œ ë¬´ë ¤ $O(V^t)$ì˜ ì‹œê°„ì´ ê±¸ë¦¬ë©°, ì„±ëŠ¥ìƒ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ê°€ ë§ë‹¤.</p>

<hr />

<p>ìœ„ ë‘ê°€ì§€ ë°©ë²•ì„ ì ˆì¶©ì•ˆ ë°©ì•ˆì´ ì„¸ë²ˆì§¸ ë°©ì•ˆì¸</p>

<ol>
  <li>Beam search</li>
</ol>

<p>ê°€ì¥ í™•ë¥ ì´ ë†’ì€ beam size kê°œ ë§Œí¼ì˜ ê°€ì§€ì¹˜ê¸°ë¥¼ í•˜ì—¬ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´(ë˜ëŠ” hypothese)ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì´ë‹¤.<br />
\(score(y_1,\dots,y_t)=logP_{LM}(y_1,\dots,y_t|x)=\sum_{i=1}^tlogP_{LM}(y_i|y_1,\dots,y_{i-1},x)\)</p>

<p><strong>[math. ê°€ì¹˜ì¹˜ê¸°ë¡œ ìƒì„±ëœ ê²½ìš°ì˜ ìˆ˜ì˜ í™•ë¥ ì„ êµ¬í•˜ëŠ” ìˆ˜ì‹]</strong></p>

<ul>
  <li>0~1 ì‚¬ì´ì¸ í™•ë¥  ê°’ì— log í•¨ìˆ˜ë¥¼ ì”Œì›Œ ë§ì…ˆìœ¼ë¡œ ê³„ì‚°í•˜ê²Œ í•˜ì—¬ ê³„ì‚° ìš©ì´ + ë„ˆë¬´ ì‘ì€ ìˆ˜ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒ ë§‰ìŒ</li>
  <li>ë˜í•œ hypothesisì˜ í™•ë¥  ê°’ì´ 0~1ì‚¬ì´ ì´ë¯€ë¡œ ìŒìˆ˜ë“¤ì´ ë‚˜ì˜¤ê²Œ ë˜ë©°, ì´ê°’ë“¤ì˜ í•©ì´ ê°€ì¥ í° ê°’ì´ ê°€ì¥ ì¢‹ì€ ê°’ì´ë‹¤.</li>
</ul>

<p>beam sizeì¸ kë¥¼ ì¡°ì ˆí•˜ì—¬ ì›í•˜ëŠ” ì„±ëŠ¥ì— íƒ€í˜‘í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, ì´ ë°©ë²•ì€ ìµœì ì˜ ê²°ê³¼ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.</p>

<h4 id='Beam-searchì˜-ì˜ˆì‹œ'>Beam searchì˜ ì˜ˆì‹œ</h4>

<p>kê°€ 2ì¼ ë•Œ, Referenceê°€ â€œ<SoS> he hit me with a pie<EoS>" ì¸ ë¬¸ì¥ì˜ ê²½ìš°</EoS></SoS></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210218141434736.png" alt="" /></p>

<p><strong>[img. Beam Searching ê³¼ì •]</strong></p>

<ul>
  <li>[he, hit, me]ê¹Œì§€ ì§„í–‰í•œ -2.5ê°€ ìµœê³  ì ìˆ˜ì„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<h4 id='Beam-searchì˜-ì¢…ë£Œì™€-ì ìˆ˜-í‰ê°€'>Beam searchì˜ ì¢…ë£Œì™€ ì ìˆ˜ í‰ê°€</h4>

<p>Greedy decoding(algorithm)ì˜ ê²½ìš° <END> í† í°ì´ ë‚˜ì˜¬ ê²½ìš°, ì˜ˆì¸¡ì˜ ì¢…ë£Œì„ì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.</END></p>

<p>í•˜ì§€ë§Œ beam searchì˜ ê²½ìš°, kê°€ 1 ì´ˆê³¼ì¼ ê²½ìš°, ê°€ì§€ì¹˜ê¸°ë¡œ <END>ê°€ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ê°€ì§€(hypothesis)ê°€ ê³„ì† ë»—ê²Œ ëœë‹¤.</END></p>

<p>ê·¸ëŸ¬ë¯€ë¡œ, ì˜ˆì¸¡ì˜ ì¢…ë£Œë¥¼ ìœ„í•´ ìµœëŒ€ Time step(ë˜ëŠ” ë¬¸ì¥ì˜ ê¸¸ì´) Të¥¼ ì •í•˜ê³ , ê·¸ ì´ìƒ ë¶€í„°ëŠ” ì˜ˆì¸¡ í•˜ì§€ ì•Šê±°ë‚˜, ë˜ëŠ” nê°œì˜ ì¢…ë£Œëœ ê°€ì§€(hypothesis), ì¦‰ nê°œì˜ <END> í† í°ì´ ë‚˜ì˜¬ë•Œ ê¹Œì§€ë§Œ ì˜ˆì¸¡ì„ ì§„í–‰í•˜ê²Œ í•œë‹¤.</END></p>

<ul>
  <li>ì—¬ê¸°ì„œ Tì™€ nì€ predefined, ì¦‰ ë¯¸ë¦¬ ì •ì˜í•´ ì¤˜ì•¼í•œë‹¤.</li>
  <li>ì¢…ë£Œë  ë•Œê¹Œì§€, <END> í† í°ì´ ë‚˜ì™€ ì¢…ë£Œëœ ê°€ì§€ëŠ” ë”°ë¡œ ë§ˆë ¨í•œ ì €ì¥ê³µê°„ì— ì ìˆ˜ì™€ ë‚´ìš©ì„ ì €ì¥í•´ ë†“ê³ , ì¢…ë£Œëœ í›„, ì ìˆ˜ë¥¼ ë¹„êµí•˜ê²Œ ëœë‹¤.</END></li>
</ul>

<p>ì´ë•Œ, ê·¸ì € ì ìˆ˜ë¥¼ ë¹„êµí•˜ë©´, ìƒëŒ€ì ìœ¼ë¡œ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ì§§ì€ ê²½ìš°ê°€ ìœ ë¦¬í•˜ê²Œ ë˜ë¯€ë¡œ, ë¬¸ì¥ì˜ ê¸¸ì´ë¡œ ë‚˜ëˆ„ì–´ ì£¼ì–´, ì „ì²´ ë‹¨ì–´ì˜ í‰ê·  í™•ë¥ ì´ ë†’ì€ ê°€ì§€ë¥¼ ê³ ë¥´ê²Œ í•œë‹¤.</p>

<p>\(score(y_1,\dots,y_t)=\frac{1}{t}\sum_{i=1}^tlogP_{LM}(y_i|y_1,\dots,y_{i-1},x)\\where\ t = number\ of\ hypothese\)<br />
<strong>[math. ê¸¸ì´ Normalizeê°€ ì ìš©ëœ score ê³„ì‚°ë²•]</strong></p>

<h3 id='BLEU-score'>BLEU score</h3>

<ul>
  <li>ìì—°ì–´ ìƒì„± ê²°ê³¼ì˜ í’ˆì§ˆì˜ ì²™ë„ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.</li>
</ul>

<p>ë‹¨ìˆœíˆ ë¬¸ì¥ì˜ Indexë¼ë¦¬ ë¹„êµë¥¼ í•˜ë©´, ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ë‹¤ë¥¼ ê²½ìš° 0ì ìœ¼ë¡œ í‰ê°€ë  ìˆ˜ ìˆë‹¤.</p>

<table>
  <thead>
    <tr>
      <th>Â </th>
      <th>ë¬¸ì¥</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Reference</td>
      <td>I love you baby, and itâ€™s a quite alright</td>
    </tr>
    <tr>
      <td>Predicted</td>
      <td>oh, I love you baby, and itâ€™s a quite alright</td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. â€œohâ€ í•œ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ Indexê°€ ë’¤ë¡œ ë°€ë¦° ê²½ìš°ì˜ ë¬¸ì¥]</strong></p>

<p>ì´ë¥¼ ìœ„í•´ ë‹¨ìˆœë¹„êµ ì´ì™¸ì˜ í‰ê°€ë°©ë²•ë“¤ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.</p>

<h4 id='ì •ë°€ë„-precision-ì¬í˜„ìœ¨-recall-ì¡°í™”í‰ê· -F-measure'>ì •ë°€ë„(precision), ì¬í˜„ìœ¨(recall), ì¡°í™”í‰ê· (F-measure)</h4>

<p>ì£¼ì–´ì§„ ë¬¸ì¥ì´</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Reference: Half of my heart is in Havana ooh na na

Predicted: Half as my heart is in Obama ooh na, 

</code></pre></div></div>
<p>ì¼ë•Œ,<br />
\(precision=\frac{\#(correct\ words)}{length\_of\_prediction}=\frac{7}{9}=78\%\\
recall=\frac{\#(correct\ words)}{length\_of\_reference}=\frac{7}{10}=70\%\\
F-measure=\frac{precision\times recall}{\frac{1}{2}(precision+recall)}=\frac{0.78\times0.7}{0.5\times(0.78+0.7)}=73.78\%\)<br />
<strong>[math. ì£¼ì–´ì§„ ë¬¸ì¥ì— ëŒ€í•œ ì •ë°€ë„, ì¬í˜„ìœ¨, ì¡°í™”í‰ê·  ]</strong></p>

<ul>
  <li><strong>ì •ë°€ë„(precision)</strong>ëŠ” ê²€ìƒ‰ëœ ê²°ê³¼ë“¤ ì¤‘ ê´€ë ¨ ìˆëŠ” ê²ƒìœ¼ë¡œ ë¶„ë¥˜ëœ ê²°ê³¼ë¬¼ì˜ ë¹„ìœ¨ì´ê³ , <strong>ì¬í˜„ìœ¨(recall)</strong>ì€ ê´€ë ¨ ìˆëŠ” ê²ƒìœ¼ë¡œ ë¶„ë¥˜ëœ í•­ëª©ë“¤ ì¤‘ ì‹¤ì œ ê²€ìƒ‰ëœ í•­ëª©ë“¤ì˜ ë¹„ìœ¨ì´ë‹¤.</li>
  <li>ì‚°ìˆ  í‰ê·  $\geq$ ê¸°í•˜ í‰ê·  $\geq$ ì¡°í™” í‰ê· ì´ ì„±ë¦½í•˜ë¯€ë¡œ, ì˜¤ë¥˜ì— ì¢€ë” ê°€ì¤‘ì„ ì£¼ê¸° ìœ„í•´ ì¡°í™”í‰ê· ì„ ì‚¬ìš©í•œë‹¤.
    <ul>
      <li>ì‚°ìˆ  í‰ê·  : (a + b) / 2</li>
      <li>ê¸°í•˜ í‰ê· : $(a*b)^\frac{1}{2}$</li>
      <li>ì¡°í™” í‰ê· : $\frac{1}{\frac{\frac{1}{a}+\frac{1}{b}}{2}}$</li>
    </ul>
  </li>
</ul>

<p>í•˜ì§€ë§Œ ì´ ì²™ë„ëŠ” Sequence dataì˜ ìˆœì„œì˜ ì˜¤ë¥˜ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•„ ë¶€ì ì ˆí•˜ë‹¤.</p>

<p>ì˜ˆë¥¼ ë“¤ìë©´, ì£¼ì–´ì§„ ë¬¸ì¥ì´</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Reference: Half of my heart is in Havana ooh na na

Model 1 Predicted: Half as my heart is in Obama ooh na, 

Model 2 Predicted: Havana na in heart my is Half ooh of na, 

</code></pre></div></div>
<p>ì¼ë•Œ,</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Model 1</th>
      <th>Model 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Precision</td>
      <td>78%</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>Recall</td>
      <td>70%</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>F-measure</td>
      <td>73.78%</td>
      <td>100%</td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. ì„¸ê°€ì§€ ì²™ë„ë¡œ í‰ê°€ ì‹œ ì˜ëª»ë˜ëŠ” ì˜ˆì‹œ]</strong></p>

<p>ì ì ˆí•˜ì§€ ëª»í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<h4 id='BLEU-score'>BLEU score</h4>

<p>BiLingual Evaluation understudy(BLEU)ëŠ” ìì—°ì–´ ì²˜ë¦¬ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡Œë‹¤.</p>

<p>\(BLEU=min(1,\frac{length\_of\_prediction}{length\_of\_reference})(\prod^4_{i=1}precision_i)^\frac{1}{4}\)<br />
<strong>[math.  BLEU ê³„ì‚° ìˆ˜ì‹]</strong></p>

<ul>
  <li>ê¸°í•˜í‰ê· ì„ ì´ìš©í•˜ì—¬ ì¡°í™”í‰ê·  ë³´ë‹¤ëŠ” ì˜¤ë¥˜ì— ê´€ëŒ€í•˜ê²Œ í•˜ì˜€ë‹¤.</li>
  <li>N-gram overlapì„ ì´ìš©í•˜ì—¬ ë‹¨ì–´ì˜ ìˆœì„œ ë˜í•œ í‰ê°€ì— ë°˜ì˜í•˜ê²Œ í•˜ì˜€ë‹¤.</li>
  <li>recall ëŒ€ì‹  precisionì„ í‰ê°€ì— ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”, ê¸°ê³„ ë²ˆì—­ ë“±ì—ì„œëŠ”  ë‹¨ì–´ì˜ ìˆ˜, ë¬¸ì¥ì˜ ê¸¸ì´ ë“±ì´ ì •í™•íˆ ë§ì§€ ì•Šì•„ë„ ì˜¬ë°”ë¥¸ ê²°ê³¼ì¸ ê²½ìš°ê°€ ìˆê¸° ë•Œë¬¸ì—, referenceì˜ ê¸¸ì´ì— ê°•ìš”ë°›ì§€ ì•Šê¸° ìœ„í•´ì„œ ì´ë‹¤.</li>
  <li>ex) ë‚˜ëŠ” ì •ë§ ë‹ˆê°€ ë§ì´ ì¢‹ì•„ , ë‚œ ì •ë§ ë‹ˆê°€  ì¢‹ì•„ :arrow_right: ê¸¸ì´ê°€ ë‹¤ë¥´ì§€ë§Œ ë‘˜ë‹¤ ì˜³ì€ ë²ˆì—­ì´ë‹¤.</li>
  <li>ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ì§§ì€ ê²½ìš° ì˜ë¯¸ë¥¼ ëª¨ë‘ ë‹´ì§€ ì•Šì€ ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ brevity penaltyë¥¼ ì£¼ì§€ë§Œ, ê·¸ë ‡ë‹¤ê³  í•´ì„œ ê²°ê³¼ê°’ì´ ê¸¸ìˆ˜ë¡ ì ìˆ˜ê°€ ë†’ì•„ì§€ëŠ” ê²ƒì„ ë§‰ê¸°ìœ„í•´ min í•¨ìˆ˜ë¥¼ ì”Œì›Œ 1ì´ ìµœëŒ€ê°’ìœ¼ë¡œ ì£¼ê²Œ í•˜ì˜€ë‹¤.</li>
</ul>

<h4 id='BLEU-ê³„ì‚°-ì˜ˆì‹œ'>BLEU ê³„ì‚° ì˜ˆì‹œ</h4>

<p>ì£¼ì–´ì§„ ë¬¸ì¥ì´</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Reference: Half of my heart is in Havana ooh na na

Model 1 Predicted: Half as my heart is in Obama ooh na, 

Model 2 Predicted: Havana na in heart my is Half ooh of na, 

</code></pre></div></div>
<p>ì¼ë•Œ,</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Model 1</th>
      <th>Model 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Precision (1-gram)</td>
      <td>7/9</td>
      <td>10/10</td>
    </tr>
    <tr>
      <td>Precision (2-gram)</td>
      <td>4/8</td>
      <td>0/9</td>
    </tr>
    <tr>
      <td>Precision (3-gram)</td>
      <td>2/7</td>
      <td>0/8</td>
    </tr>
    <tr>
      <td>Precision (4-gram)</td>
      <td>1/6</td>
      <td>0/7</td>
    </tr>
    <tr>
      <td>Brevity penalty</td>
      <td>9/10</td>
      <td>10/10</td>
    </tr>
    <tr>
      <td>BLEU</td>
      <td>$0.9\times (1/54)^{\frac{1}{4}} \approx 33\%$</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. BLEU ê³„ì‚° ì˜ˆì‹œ]</strong></p>

<h2 id='Tansformer'>Tansformer</h2>

<blockquote>
  <p>RNNì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  attention ë§Œìœ¼ë¡œ Sequetial dataë¥¼ ì…ë ¥ ë°›ê³  ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸</p>
</blockquote>

<h3 id='Transform-introduction'>Transform introduction</h3>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219113408772.png" alt="" /></p>

<p><strong>[img. Transform ëª¨ë¸ êµ¬ì¡°]</strong></p>

<p>Transformê³¼ Attention ëª¨ë¸ì€ ê°™ì€ ë…¼ë¬¸ì—ì„œ ì²˜ìŒ ì†Œê°œë˜ì—ˆìŒ.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219110855868.png" alt="" />*</p>

<p><strong>[img. ê¸°ì¡´ì˜ RNN]</strong></p>

<p>ê¸°ì¡´ì˜ ì™¸ë°©í–¥ RNNì€  ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œê°€ ìˆì—ˆë‹¤.</p>

<ul>
  <li>ì „ë‹¬ë˜ëŠ” hidden Stateê°€ ìŒ“ì¼ ìˆ˜ë¡ ë³‘ëª© í˜„ìƒ(bottle neck) ë•Œë¬¸ì— ë©€ë¦¬ ìˆëŠ” ê³¼ê±° ì •ë³´(context)ê°€ ë³€í˜•ë˜ê±°ë‚˜ ì†Œì‹¤ ë˜ëŠ” ë¬¸ì œ</li>
  <li>ë°˜ëŒ€ ë°©í–¥ Back propagation ì‹œ Vanishing/Exploding gradient Problemì„ ê°€ì§€ê³  ìˆì—ˆë‹¤.</li>
  <li>Sequential data ìˆœì„œ ë°˜ëŒ€ ë°©í–¥(ì¦‰, ë¯¸ë˜ì— ë‚˜ì˜¬)ì˜ ì •ë³´ëŠ” í•´ë‹¹ Time stepì—ì„œ ì°¸ê³ í•  ìˆ˜ ì—†ìŒ.</li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219113428347.png" alt="" /></p>

<p><strong>[img. Bi-directional RNN]</strong></p>

<p>ì–‘ë°©í–¥ RNN(Bi-directional RNN)ì€ ë³„ê°œì˜ ë‹¤ë¥¸ ë°©í–¥ì˜ RNN ëª¨ë¸ 2ê°œë¥¼ ìƒì„±í•˜ì—¬, ê° Time stepì— 2ê°œì˜ Hidden stateë¥¼ Concatí•˜ì—¬ 2ë°°ì˜ Dimensionì„ ê°€ì§„ Vectorë¥¼ ê°€ì§€ëŠ” ëŒ€í‘œì ì¸ Encoding Vectorë¥¼ í˜•ì„±í•œë‹¤.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219113626261.png" alt="" /></p>

<p><strong>[img. Traonsform attention êµ¬ì¡°ì—ì„œ context vectorë¥¼ êµ¬í•˜ëŠ” ê³¼ì •]</strong></p>

<p>Encoderì™€ Decoder ë‚´ë¶€ì—ì„œëŠ” ì•ì„œ ë°°ì› ë˜ Attention ëª¨ë“ˆê³¼ ë¹„ìŠ·í•œ Self-attention ëª¨ë“ˆì„ í†µí•˜ì—¬ Output vectorë¥¼ í˜•ì„±í•˜ê²Œ ëœë‹¤.</p>

<p>ì•ì„  Attention ëª¨ë“ˆê³¼ì˜ ì°¨ì´ì ì€ Self-attentionì€ Decoderì—ì„œ Encoderì˜ hidden stateë¥¼ ìê¸° ìì‹ ì˜ Input vectorì™€ ì‚¬ìš©í•œ ê²ƒê³¼ ë‹¤ë¥´ê²Œ ìê¸° ìì‹ ì˜ Input Vectorë§Œìœ¼ë¡œ Output vectorë¥¼ í˜•ì„±í•œë‹¤ëŠ” ì ì´ë‹¤.</p>

<p>ì´ë•Œ Self Attention êµ¬ì¡°ê°€ ì•„ë‹Œ, ë‹¨ìˆœ Input Vectorì˜ ë‚´ì ìœ¼ë¡œ êµ¬ì„±ì‹œ, ìê¸° ìì‹ ì˜ Inputì—ë§Œ ê³¼ë„í•˜ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë‹¨ì ì´ ìˆë‹¤ê³  í•œë‹¤.</p>

<p>Self-Attention ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<ol>
  <li>ê°ì Embeddingëœ Input Vectorë“¤($X_1,\dots,X_t$)ë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ Xë¥¼ $W^Q$ì™€ ë‚´ì í•˜ì—¬ Query Vector ë“¤ë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ Që¥¼ êµ¬í•œë‹¤.</li>
  <li>ê°™ì€ í–‰ë ¬ Xë¥¼ $W^K$ì™€ ë‚´ì í•˜ì—¬ Key Vectorë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ Kë¥¼ êµ¬í•œë‹¤.</li>
  <li>
    <p>ê°™ì€ í–‰ë ¬ Xë¥¼ $W^V$ì™€ ë‚´ì í•˜ì—¬ Values Vectorë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ Vë¥¼ êµ¬í•œë‹¤.</p>
  </li>
  <li>
    <p>Qì™€ $K^T$ì˜ ë‚´ì í•œ ê²°ê³¼ì— Softmax í•¨ìˆ˜ë¥¼ ì²˜ë¦¬ í•œ ë’¤,  Vë¥¼ ë‚´ì í•˜ì—¬ Context Vectorë“¤ë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬  Zë¥¼ êµ¬í•œë‹¤.</p>

    <ul>
      <li>
        <p>ZëŠ” Value Vectorë“¤ì˜ Weighted Sums</p>
      </li>
      <li>$K^T$ëŠ” Qì™€ ë‚´ì í•˜ê¸° ìœ„í•´ K í–‰ë ¬ì˜ ì°¨ì› ì„ Transform í•œê²ƒ. $(a\times b)\rightarrow(b\times a)$</li>
      <li>ì´ë•Œ Qì™€ Kì˜ ì°¨ì› ìˆ˜($d_k)$ëŠ” ê°™ì•„ì•¼ í•˜ê³ , Vì˜ ì°¨ì›ìˆ˜($d_v$)ëŠ” ë‹¬ë¼ë„ ëœë‹¤.
        <ul>
          <li>í•˜ì§€ë§Œ ë³´í†µì€ í¸ì˜ë¥¼ ìœ„í•´ ì¼ë¶€ëŸ¬ ì°¨ì›ìˆ˜ë¥¼ ëª¨ë‘ ê°™ê²Œ í•œë‹¤.</li>
        </ul>
      </li>
      <li>ê·¸ì € ë‚´ì ë§Œ í•˜ì§€ ì•Šê³ , kì˜ ì°¨ì›ìˆ˜(=qì˜ ì°¨ì›ìˆ˜)ì˜ ë£¨íŠ¸ê°’ìœ¼ë¡œ ë‚˜ëˆ ì£¼ëŠ” ë°©ë²•ë„ ìˆë‹¤. ì•„ë˜ Scaled Dot-Product Attention ì°¸ì¡°</li>
    </ul>
  </li>
</ol>

<p>ì´ ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.<br />
\(A(q,K,V)=\sum_i\frac{\exp(q\cdot k_i)}{\sum_j\exp(q\cdot k_j)}v_i\\
A(Q,K,V)=softmax(QK^T)V\\
where\ Q: query\ vectorê°€\ ëª¨ì¸\ í–‰ë ¬, K^T: Kì˜\ ì°¨ì›ì„\ Transform\)</p>

<p><strong>[math. Dot-Product Attentionì˜ ìˆ˜ì‹ê³¼ ê°„ë‹¨í•œ ë²„ì „]</strong></p>

<p>| <img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219113918362.png" alt="" /> |<br />
| :â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-: |<br />
| $(|Q|\times d_k)\cdot s(d_k\times |K|)\cdot(|V|\times d_v)=(|Q|\times d_v)$ |<br />
<strong>[fig. Self-attention ê³¼ì •ì„ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒ]</strong></p>

<ul>
  <li>softmax í•¨ìˆ˜ì˜ ê¸°ë³¸ ìˆ˜í–‰ì€ Row-Wiseì´ë‹¤.</li>
</ul>

<p>| ìˆ˜ì‹                                         | ë„ì‹                                                         |<br />
| â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ | â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” |<br />
| $A(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$ | <img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219114003687.png" alt="" /> |<br />
<strong>[fig. Scaled Dot-Product Attention ]</strong></p>

<p>ì´ë•Œ kì˜ ì°¨ì›ìˆ˜ê°€ ì»¤ì§€ë©´ Qì™€ Kì˜ ë‚´ì ê°’ì˜ ë¶„ì‚°ê°’ì´ ì»¤ì§€ê²Œ ë˜ê³ , ì´ë ‡ê²Œ ë¶„ì‚°ê°’ì´ í° Vectorì— Softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ê²°ê³¼ë¡œ í™•ë¥ ë¶„í¬ ë˜í•œ ê·¹ë‹¨ì ìœ¼ë¡œ ë‚˜ì˜¤ê²Œ ë˜ë©°, ì´ë ‡ê²Œ ê·¹ë‹¨ì ìœ¼ë¡œ ë‚˜ì˜¨ í™•ë¥ ë¶„í¬ëŠ” Vanishing Gradient Problemì„ ì¼ìœ¼í‚¨ë‹¤.</p>

<p>ì´ë¥¼ ë§‰ê¸° ìœ„í•´ ë‚´ì ê°’ë“¤ì— softmax í•¨ìˆ˜ ì´ì „ì— $\sqrt{d_k}$ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì‚°ì„ ì¤„ì´ëŠ” ë°©ë²•ì„ Scaled Dot-Product Attentionì´ë¼ê³  í•œë‹¤.</p>

<h3 id='Transformer-contâ€™d'>Transformer(contâ€™d)</h3>

<h4 id='Multi-Head-Attention'>Multi-Head Attention</h4>

<blockquote>
  <p>Self-Attention ëª¨ë“ˆì„ ì¢€ë” ìœ ì—°í•˜ê²Œ í™•ì¥í•œ ëª¨ë“ˆ</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>ìˆ˜ì‹</th>
      <th>ë„ì‹</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$MultiHead(Q,K,V)=Concat(head_1,\dots,head_k)W^O\where\ head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$</td>
      <td><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219141716766.png" alt="" /></td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. Multi-Head Attention]</strong></p>

<p>Multi-Head Attentionì€ ì•ì„œì‚¬ìš©í•œ Scaled Dot-Productë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ê°ê¸° ë‹¤ë¥¸ ì—¬ëŸ¬ Parameter($W^Q_i,W^K_i,W^V_i$)ì„ ì´ìš©í•´ ì—¬ëŸ¬ ê²¹ìœ¼ë¡œ ì§„í–‰í•œ ëª¨ë¸ì´ë‹¤.</p>

<p>ìœ„ì˜ Scaled Dot-product Attentionì˜ $W^Q, W^K, W^V$ì™€ ë‹¤ë¥¸ ë“¯?<br />
<img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219142435145.png" alt="" /></p>

<p><strong>[img. ê° í—¤ë“œë³„ Attention ê²°ê³¼]</strong></p>

<p>ì´ë•Œ ê° ì¸µì„ Attention Headë¼ê³  í•˜ë©°, ê°ê¸° ë‹¤ë¥¸ Output Vector $Z_i$ê°€ ë‚˜ì˜¤ê²Œ ëœë‹¤.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219142502169.png" alt="" /></p>

<p><strong>[img. ìµœì¢… Ouput Vector Zì˜ ë„ì¶œ]</strong></p>

<p>ê° Attention Headì— ë‚˜ì˜¨ Output Vectorë¥¼ 1ê°œì˜ ë‹¤ì°¨ì› ë²¡í„°ë¡œ Concat í•œ ë’¤, ì¶”ê°€ë¡œ ì„ í˜• ë³€í™˜í•˜ì—¬ ìµœì¢… Output Vector Zë¥¼ êµ¬í•˜ê²Œ ëœë‹¤.</p>

<p>| Layer Type                 | Complexity per Layer   | Sequential Operations | Maximum Path Length |<br />
| â€”â€”â€”â€”â€”â€”â€”â€”â€“ | â€”â€”â€”â€”â€”â€”â€”- | â€”â€”â€”â€”â€”â€”â€” | â€”â€”â€”â€”â€”â€”- |<br />
| Self-Attention             | $O(n^2\cdot d)$        | $O(1)$                | $O(1)$              |<br />
| Recurrent                  | $O(n\cdot d^2)$        | $O(n)$                | $O(n)$              |<br />
| Convolutional              | $O(k\cdot n\cdot d^2)$ | $O(1)$                | $O(\log_k(n))$      |<br />
| Self-Attention(restricted) | $O(r\cdot n\cdot d)$   | $O(1)$                | $O(n/r)$            |</p>
<ul>
  <li>n : ì…ë ¥ Sequence data ê¸¸ì´</li>
  <li>d :  hyper parameter ì°¨ì›</li>
  <li>k : CNN kernel Size</li>
  <li>r : ìµœëŒ€ ì´ì›ƒê±°ë¦¬ (restricted self-attention)</li>
</ul>

<p><strong>[fig. ëª¨ë¸ì— ë”°ë¥¸ ê³„ì‚° ë³µì¡ë„ ë„í‘œ]</strong></p>

<p>ê° ì¸µì—ì„œì˜ ì—°ì‚°ì—ì„œ Self-Attention êµ¬ì¡°ëŠ” Recurrentì™€ ë‹¤ë¥´ê²Œ Query vectorì™€ Key vectorì˜ ë‚´ì ì„ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ O($n^2\cdot d$)ë§Œí¼ì˜ ì„±ëŠ¥ì´ ê±¸ë¦¬ê³  RecurrentëŠ” ê° inputë§ˆë‹¤ ìˆœì°¨ì ìœ¼ë¡œ ì¼ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ O($n\cdot d^2$)ë§Œí¼ì˜ ì‹œê°„ì´ ê±¸ë¦°ë‹¤.</p>

<p>ì¼ë°˜ì ìœ¼ë¡œ nì€ ì…ë ¥ ë°ì´í„°ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ ë³€í•˜ë¯€ë¡œ Self-Attention êµ¬ì¡°ì˜ ë³€ë™ì„±ì´ ë” í¬ë‹¤ê³  í•  ìˆ˜ ìˆì§€ë§Œ, ì´ì „ Hidden stateê°€ Inputìœ¼ë¡œ ì£¼ì–´ì ¸ì•¼ ë‹¤ìŒ Hidden stateë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì–´ $O(n)$ì˜ ì‹œê°„ì´ ë“œëŠ” RNNê³¼ ë‹¬ë¦¬ Self-Attention êµ¬ì¡°ëŠ” ë™ì‹œì— ì§„í–‰í•  ìˆ˜ ìˆì–´, ì»´í“¨í„° ì½”ì–´ ìˆ˜ë§Œ ë§ë‹¤ë©´ ë³‘ë ¬ì ìœ¼ë¡œ ìƒìˆ˜ ì‹œê°„ ë‚´ì— ì§„í–‰í•  ìˆ˜ ìˆì–´ ë¹ ë¥´ë‹¤.</p>

<p>ë˜í•œ Back propagation ì§„í–‰ì‹œ, ì •ë³´ ì ‘ê·¼ ì‹œì—ë„ ë¬´ì¡°ê±´ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰ë˜ëŠ” RNNê³¼ ë‹¬ë¦¬ ê³§ë°”ë¡œ íŒŒë¦¬ë¯¸í„°ì— ì ‘ê·¼í•  ìˆ˜ ìˆì–´ì„œ Self-Attention êµ¬ì¡°ê°€ ë”ìš± ë¹ ë¥´ë‹¤.</p>

<h4 id='Self-Attention-Blockê³¼-Residual-Connection'>Self-Attention Blockê³¼ Residual Connection</h4>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219113313354.png" alt="" /></p>

<p><strong>[img. í•˜ë‚˜ì˜ Self-Attention block]</strong></p>

<p>ì´ëŸ¬í•œ Attention êµ¬ì¡°ì˜ ê³µí†µì ì¸ Attention ê³¼ì •ê³¼ í›„ì²˜ë¦¬ ë¶€ë¶„ì„ ë¸”ë¡í™”í•˜ì—¬ í•˜ë‚˜ì˜ ëª¨ë“ˆì²˜ëŸ¼ ì‚¬ìš©í•˜ê³¤ í•œë‹¤. (ì •í™•íˆëŠ” Encoder ë¶€ë¶„)</p>

<ul>
  <li>í¬ê²Œ Feed-forward ë¶€ë¶„ê³¼ Multi-head attention ë‘ êµ¬ì¡°ë¡œ ë‚˜ë‰œë‹¤.</li>
</ul>

<p>ì´ë•Œ CNNì—ì„œ ë³¸ ê²ƒê³¼ ê°™ì´  Input ê°’ì„  Attention ì²˜ë¦¬í•˜ì§€ ì•Šê³  Skipí•˜ì—¬ ì™„ì„±ëœ Output Vectorê°’ì— í–‰ë ¬í•©ì„ í•´ì£¼ëŠ” ê²ƒì„ Residual connectionì´ë¼ê³  í•˜ë©° í•œ Blockì— 2ë²ˆ ì²˜ë¦¬ëœë‹¤.<br />
\(LayerNorm(x+sublayer(x))\)<br />
<strong>[math. Residual connection ìˆ˜ì‹]</strong></p>

<p>CNNê³¼ ê°™ì€ ì¥ì ì¸ Vanishing Gradient Problemì„ í•´ê²°í•  ìˆ˜ ìˆë‹¤.</p>

<p>ì´ë•Œ ì¤‘ìš”í•œ ì ì€ í–‰ë ¬ê°„ì˜ ë§ì…ˆì´ ì„±ë¦½ë˜ê¸° ìœ„í•´ Attention ì²˜ë¦¬ëœ Output VectorëŠ” ì›ë³¸ Inputê³¼ ê°™ì€ ì°¨ì›ì„ ê°€ì ¸ì•¼ í•œë‹¤ëŠ” ì ì´ë‹¤.</p>

<h4 id='Layer-Normalization'>Layer Normalization</h4>

<p>í•™ìŠµ ì•ˆì •í™”ì™€ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ Layer Normalizationì„ í•œë‹¤.</p>

<p>í¬ê²Œ 2ë‹¨ê³„ë¡œ ë‚˜ë‰˜ëŠ”ë°,</p>

<ol>
  <li>Vectorë“¤ì˜ í‰ê· ê³¼ ë¶„ì‚°ê°’ì„ 0ê³¼ 1ë¡œ ë°”ê¾¸ëŠ” Normalization ê³¼ì •</li>
  <li>í•™ìŠµê°€ëŠ¥í•œ Parameterë¥¼ ì§‘ì–´ë„£ê¸° ìœ„í•´ íŒŒë¼ë¯¸í„°ê°€ í¬í•¨ëœ ì„ í˜• í•¨ìˆ˜ì— ì§‘ì–´ë„£ëŠ” Affine Transformation ê³¼ì •</li>
</ol>

<p>ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤.</p>

<p>Residual Connectionê³¼ ê°™ì´ 2ë²ˆ ì‹œí–‰ëœë‹¤.<br />
\(\mu^l=\frac{1}{H}\sum^H_{i=1}a^l_i,\ \ \sigma^l=\sqrt{\frac{1}{H}\sum^H_{i=1}(a_i^l-\mu^l)^2},\ \ h_i=f(\frac{g_i}{\sigma_i}(a_i-\mu_i)+b_i)\\
\mu : í‰ê· ,\ \sigma : í‘œì¤€\ í¸ì°¨,\ h_i:Result\ Vector\)</p>

<p><strong>[math. Layer Normalizationì˜ ìˆ˜ì‹]</strong></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219112750089.png" alt="" /></p>

<p><strong>[img. Layer Normalization ì´ì™¸ì˜ ë°©ë²•ë“¤]</strong></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219113244986.png" alt="" /></p>

<p><strong>[img. Layer Normalization ì˜ˆì‹œ]</strong></p>

<h4 id='Positional-Encoding'>Positional Encoding</h4>

<p>RNNê³¼ ë‹¬ë¦¬ Attentionì€ Hidden stateê°€ ìˆœì°¨ì ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•Šê³ , ëª¨ë“  hidden stateë¥¼ ë™ì‹œì— ì´ìš©í•  ìˆ˜ ìˆë‹¤.</p>

<p>ì´ ë•Œë¬¸ì— ë³‘ë ¬ ì²˜ë¦¬, Gradient ë¬¸ì œ, ë³‘ëª© ë¬¸ì œ í•´ê²° ë“± ì—¬ëŸ¬ ì¥ì ì„ ê°€ì§€ê³  ìˆì§€ë§Œ, í•˜ë‚˜ì˜ ëª¨ìˆœì´ ìˆë‹¤.</p>

<p>ë°”ë¡œ, Sequential dataì˜ ìˆœì„œë¥¼ í•™ìŠµê³¼ì • ì¤‘ì— ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ë‹¤.</p>

<ul>
  <li>RNNì˜ ê²½ìš°, ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” hidden stateëŠ” ë°”ë¡œ ì´ì „ hidden stateì´ë¯€ë¡œ, ê°ê¸° outputì´ ì£¼ì–´ì§„ ì •ë³´ê°€ ë‹¤ë¥´ë¯€ë¡œ ìˆœì°¨ì„±ì— ë”°ë¥¸ êµ¬ë³„ì´ ê°„ë‹¤.(ë’¤ë¡œ ê°ˆìˆ˜ë¡ hidden stateì— ë§ì€ ì •ë³´ê°€ ë‹´ê¸°ë‹ˆê¹Œ)</li>
  <li>í•˜ì§€ë§Œ Attentionì€ ë™ì‹œì— ëª¨ë“  hidden state ë˜ëŠ” Inputì— ì ‘ê·¼í•˜ë¯€ë¡œ êµ¬ë³„ì´ ì—†ë‹¤.(êµ¬ë³„í•˜ê²Œ ë˜ëŠ” hidden state ë˜ëŠ” Inputë³„ ê°€ì¤‘ì¹˜ëŠ” Inputì´ ì£¼ì–´ì§„ ë’¤ë¡œ ê³„ì‚°ëœë‹¤.)</li>
</ul>

<p>ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Input ê°’ì˜ ìœ„ì¹˜ì— ë”°ë¼ Vectorì— êµ¬ë³„ì´ ê°€ëŠ” íŠ¹ë³„í•œ ê°’ì„ ë”í•´ì£¼ëŠ” ë°©ë²•ì´ Positional Encodingì´ë‹¤.<br />
\(PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)}=\cos(pos/10000^{2i/d_{model}})\\\)<br />
<strong>[math. Positional Encoding ìˆ˜ì‹]</strong></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219150022231.png" alt="" /></p>

<p><strong>[img. Positional Encoding ê·¸ë˜í”„]</strong></p>

<p>siní•¨ìˆ˜ì™€ cosí•¨ìˆ˜ ì£¼ê¸°ì˜ ì°¨ì´ì„±, ë˜, ê°™ì€ í•¨ìˆ˜ ìˆœë²ˆì—ë„ ì£¼ê¸°ë¥¼ ë‹¤ë¥´ê²Œ ì£¼ì–´ ìˆœì„œì— ë”°ë¼ íŠ¹ë³„í•œ ê°’ì„ Input vectorì— ë”í•´ì¤€ë‹¤.</p>

<p>ë³´í†µ ë§¨ì²˜ìŒ Input ì‹œì ì— í•œë²ˆë§Œ ì§„í–‰ëœë‹¤.</p>

<h4 id='Warm-up-Learning-Rate-Scheduler'>Warm-up Learning Rate Scheduler</h4>

<p>Gradient Descent, Adamë“±ì˜ ê³¼ì •ì—ì„œ Hyper parameterì¸ Learning Rate ê°’ì„ ê³ ì •í•˜ì§€ ì•Šê³  ë³€ë™ì„ ì£¼ëŠ” ë°©ë²•ì´ë‹¤.<br />
\(learning\ rate = d^{-0.5}_{model}\cdot \min(\#step^{-0.5},\#step\cdot warmup\_steps^{-1.5})\)<br />
<strong>[math. Learning rate ë³€ê²½ ìˆ˜ì‹ ]</strong></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219113154754.png" alt="" /></p>

<p><strong>[img. Learning rate ë³€ê²½ì˜ ì˜ˆì‹œ]</strong></p>

<p>ê²½í—˜ì ìœ¼ë¡œ ìœ„ì™€ ê°™ì€ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•œë‹¤.</p>

<h4 id='High-Level-View-Visualization'>High-Level View, Visualization</h4>

<p>Encoderì— Attention Blockì„ ìŒ“ì•„ì„œ High-Levelë¥¼ ë§Œë“¤ ìˆ˜ ìˆì–´ì„œ, Visualizationì„ í†µí•´ ë³¼ ìˆ˜ ìˆë‹¤</p>

<p>Decoderì˜ Multi-Head Attentionì—ì„œ Encoderì—ì„œ ê°€ì ¸ì˜¨ Key í–‰ë ¬ê³¼ Value í–‰ë ¬ì— ì¤€ ê°€ì¤‘ì¹˜ë¥¼ layerë³„ë¡œ í™•ì¸í•˜ì—¬, ëª¨ë¸ì˜ ì¶”ë¡ ì„ Visualizationí•  ìˆ˜ ìˆë‹¤.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219154605759.png" alt="" /></p>

<p><strong>[img. ê° layer ë³„ ì°¸ì¡°í•œ ë‹¨ì–´ì— ëŒ€í•œ Visualization]</strong></p>

<h4 id='Decoder-amp-Masked-Self-Attention'>Decoder &amp; Masked Self-Attention</h4>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219155103044.png" alt="" /></p>

<p><strong>[img. Encoder-Decoder êµ¬ì¡°]</strong></p>

<p>Decoder ë¶€ë¶„ì—ì„œëŠ” 2ë²ˆì˜ Self-Attentionì„ ì§„í–‰í•˜ê²Œëœë‹¤.</p>

<ol>
  <li>
    <p>Masked decoder ë¶€ë¶„</p>

    <ul>
      <li>ë‹¨ìˆœ ì¶”ë¡  ë¿ë§Œ ì•„ë‹ˆë¼ Query í–‰ë ¬ê³¼ Key í–‰ë ¬ì˜ ë‚´ì  ì—°ì‚° Softmax outputì˜ ì¼ë¶€ë¶„ì„ ë§ˆìŠ¤í‚¹í•œ ë’¤, ë‹¤ì‹œ Normalization í•˜ì—¬ ë‚´ë³´ë‚´ëŠ” ë¶€ë¶„.</li>
      <li>ì´ë¥¼ í†µí•´ ì•„ì§ ì¶”ë¡ í•˜ì§€ ì•Šì•„ ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ”(ìƒì„±ë˜ì§€ ì•Šì€) ì´í›„ì˜ Sequential dataì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ê²ƒì„ ë§‰ëŠ”ë‹¤.</li>
    </ul>
  </li>
  <li>
    <p>Encoder-Decoder ë¶€ë¶„</p>

    <ul>
      <li>Key í–‰ë ¬ê³¼ Value í–‰ë ¬ì„ Encoderì—ì„œ ê°€ì ¸ì™€ì„œ Decoderì—ì„œ ë³´ë‚´ì¤€ Query í–‰ë ¬ê³¼ Attention í•˜ëŠ” ë¶€ë¶„.</li>
    </ul>

    <p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219160108317.png" alt="" /></p>

    <p><strong>[img. Query í–‰ë ¬ê³¼ Key í–‰ë ¬ ë‚´ì ì˜ ê²°ê³¼]</strong></p>

    <p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219160636405.png" alt="" /></p>

    <p><strong>[img. ë§ˆìŠ¤í‚¹ ]</strong></p>
  </li>
</ol>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/masked_att.gif" alt="" /></p>

<p>**[gif. ì²« <SoS>í† í°ì´ ì£¼ì–´ì¡Œì„ ë•Œ, Masked Self-Attention ê³¼ì •]**</SoS></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210219112452261.png" alt="" /></p>

<p><strong>[img. Transformerì™€ ì„±ëŠ¥ ë¹„êµ]</strong></p>

<p>ë‚®ì€ ì„±ëŠ¥ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ BLEU ê¸°ì¤€ì˜ ê²°ê³¼ì´ë¯€ë¡œ, ë¹„ìŠ·í•œ ê²°ê³¼ë¬¼ë“¤ ë˜í•œ ì ìˆ˜ê°€ ë‚®ê²Œ ë‚˜ì˜¤ë¯€ë¡œ ì‹¤ì œë¡œëŠ” ìƒìš©ìœ¼ë¡œ ì“°ëŠ” ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì´ë‹¤.</p>

<h2 id='Self-supervised-Pre-training-Models'>Self-supervised Pre-training Models</h2>

<p>Self-supervised(ìê¸°ì§€ë„ í•™ìŠµ)</p>

<ul>
  <li>Taskë¥¼ í’€ê¸° ìœ„í•´ ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¼ë²¨ë§í•˜ëŠ” í•™ìŠµ</li>
</ul>

<p>Pre-training(ì‚¬ì „í›ˆë ¨)</p>

<ul>
  <li>ì—¬ëŸ¬ Taskë¡œ Transfer Learning ë  ìˆ˜ ìˆë„ë¡, í•˜ë‚˜ì˜ Taskë¡œ ëª¨ë¸ì˜ Parameterë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒ,</li>
</ul>

<p>Fine Tuning(ë¯¸ì„¸ ì¡°ì •)</p>

<ul>
  <li>ë¯¸ë¦¬ ê°œë°œ, í›ˆë ¨ëœ ëª¨ë¸ì„ ìš©ë„ì— ë§ê²Œ íŒŒë¦¬ë¯¸í„°, Layer êµ¬ì¡° ë“±ì„ ë³€ê²½í•˜ëŠ” ê²ƒ</li>
</ul>

<p>Transfer Learning(ì „ì´ í•™ìŠµ)</p>

<ul>
  <li>ë¯¸ë¦¬ ê°œë°œ, í›ˆë ¨ëœ ëª¨ë¸ì„ ìš©ë„ì— ë§ê²Œ ë‚˜ì˜ ë°ì´í„°ì…‹ ë“±ìœ¼ë¡œ ì¬í•™ìŠµì‹œí‚¤ëŠ” ê³¼ì •</li>
</ul>

<h3 id='Self-Supervised-Pre-Training-Models'>Self-Supervised Pre-Training Models</h3>

<p>Transfer Learning, Self-supervised Learning, Transformerë¥¼ ì‚¬ìš©í•´ NLPì—ì„œ ì••ë„ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ ë‘ ëª¨ë¸ì„ ì•Œì•„ë³´ì.</p>

<ul>
  <li>Transformer ëª¨ë¸ì€ ì¢‹ì€ ì„±ëŠ¥ìœ¼ë¡œ NLP ë¶„ì•¼ ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ì“°ì´ê³  ìˆë‹¤.</li>
  <li>ìµœê·¼ì—ëŠ” Self-attentio ì¸µì„ 24ì¸µ ì´ìƒìœ¼ë¡œ ìŒ“ì€ ë’¤ self-supervised learning frameworkì—ì„œ í•™ìŠµ í›„, Transfer learning í˜•íƒœë¡œ fine tuningí•œë‹¤.</li>
  <li>natural language generationì—ì„œ greedy decoding ìˆ˜ì¤€ì—ì„œ ë²—ì–´ë‚˜ì§€ ëª»í•˜ê³  ìˆë‹¤.</li>
</ul>

<h4 id='GPT-1'>GPT-1</h4>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210220190814160.png" alt="" /></p>

<p><strong>[img. GPT-1 ëª¨ë¸]</strong></p>

<p>Pretrained modelì˜ ì‹œì´ˆê²©, OPEN-AIì—ì„œ ê°œë°œ. Transformer ëª¨ë¸ì˜ êµ¬ì¡° íŠ¹íˆ Decoder êµ¬ì¡°ë¥¼ ë”°ë¦„</p>

<p>ë‹¤ì–‘í•œ special tokenì„ í†µí•´ fine-tuningê°„ì˜ transfer learningì„ íš¨ìœ¨ì ìœ¼ë¡œ ë°”ê¿ˆ</p>

<ul>
  <li>Start ë¿ë§Œ ì•„ë‹ˆë¼, Delimiter, Extranction Token ë“± ë‹¤ì–‘í•œ Special Tokenì„ í†µí•˜ì—¬ ì—¬ëŸ¬ ë¬¸ì œ í•´ê²° ê°€ëŠ¥</li>
</ul>

<p>ì—¬ëŸ¬ ìì—°ì–´ ì²˜ë¦¬(classification, similarity,  entailment ë“±)ë¥¼ í° ë³€í™” ì—†ì´ ì²˜ë¦¬ ê°€ëŠ¥í•œ í†µí•©ì ì¸ ëª¨ë¸</p>

<p>12ê°œì˜ self-attention decoder-only transformer ëª¨ë¸ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì§</p>

<p>ì—¬ëŸ¬ ë°©ì•ˆìœ¼ë¡œ ì‚¬ìš©í•  ë•ŒëŠ” ë¯¸ë¦¬ ì—¬ëŸ¬ ë°ì´í„°ë¡œ í•™ìŠµëœ GPT-1 ëª¨ë¸(pre-training)ì„ transfer learningì˜ í˜•íƒœë¡œ ì‚¬ìš© ì‹œì—ëŠ” í›„ë°˜ì˜ ì¼ë¶€ layerì„ ìš©ë„ì— ë§ê²Œ êµì²´í•˜ê³ , Learning rateë¥¼ ì ê²Œì£¼ì–´ ì¡°ê¸ˆë§Œ í•™ìŠµ ì‹œí‚¤ëŠ” ë°©ì‹(fine-tuning)ìœ¼ë¡œ ì‚¬ìš©.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210220202657899.png" alt="" /></p>

<p><strong>[img. GPT-1 ì„±ëŠ¥ ê²°ê³¼]</strong></p>

<h4 id='BERT'>BERT</h4>

<p>ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” NLP Pre-trained ëª¨ë¸, Encoder êµ¬ì¡°ë§Œ í™œìš©</p>

<p>GPTëŠ” ë‹¨ë°©í–¥ì˜ Masked Attention êµ¬ì¡°ë¥¼ í™œìš©í•˜ëŠ” ë°˜ë©´, BERTëŠ” ì•„ë˜ì˜ MLMì„ í™œìš©í•˜ì—¬ ì–‘ë°©í–¥ Self-attention êµ¬ì¡°ì´ë‹¤.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210220204134377.png" alt="" /></p>

<p><strong>[img. ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ ë¹„êµ]</strong></p>

<ul>
  <li>ELMO: Transformer ì´ì „ì— LSTM ê¸°ë°˜ Encoderë¡œ Pre-trained ëœ ëª¨ë¸
    <ul>
      <li>ìµœê·¼ì˜ ëª¨ë¸ë“¤ì€ ELMOì—ì„œ LSTM Encoderë¥¼ ëŒ€ì²´í•œ í˜•íƒœ</li>
    </ul>
  </li>
</ul>

<p>ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ì€ ì•ì— ë‚˜ì˜¨ ë‹¨ì–´ ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ë¬¸ë§¥ì„ íŒë‹¨í•˜ì§€ë§Œ, ì‹¤ì œ ë…í•´ì—ì„œëŠ” ì• ë’¤ ëª¨ë‘ì˜ ë¬¸ë§¥ì„ íŒŒì•…í•´ì•¼ í•œë‹¤.</p>

<p><strong>Masked Language Model(MLM)</strong>: BERTì—ì„œì˜ Pre-Training  í•™ìŠµ ë°©ë²• í˜•íƒœ, ë¬¸ì¥ì—ì„œ ì¼ë¶€ ë‹¨ì–´ë¥¼ [MASK] í† í°ìœ¼ë¡œ ë°”ê¾¼ ë’¤, ì•ë’¤ ë¬¸ë§¥ì„ íŒŒì•…í•˜ì—¬ ë§ì¶”ëŠ” í˜•ì‹ìœ¼ë¡œ í•™ìŠµ</p>

<ul>
  <li>ì–´ëŠ ì •ë„ ë¹„ìœ¨ì˜ ë‹¨ì–´ë“¤ì„ ë§ˆìŠ¤í‚¹í•  ê²ƒì¸ê°€?ëŠ” Hyperparameterì´ë‹¤.</li>
  <li>ë³´í†µ 15%ì •ë„ë¡œ ì‹œì‘í•˜ë©° ë„ˆë¬´ ë§ì´ ë§ˆìŠ¤í‚¹ í•˜ë©´ ë¬¸ë§¥ì´ ì œëŒ€ë¡œ íŒŒì•…í•˜ì§€ ëª»í•´ í•™ìŠµì´ ì•ˆë˜ê³ , ë„ˆë¬´ ì ê²Œ ë§ˆìŠ¤í‚¹í•˜ë©´ Trainingì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤.</li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221100405373.png" alt="" /></p>

<p><strong>[img. MLM ì˜ˆì‹œ]</strong></p>

<p>ë‹¨, Maskëœ ë‹¨ì–´ëŠ” ê·¸ëŒ€ë¡œ ë§ˆìŠ¤í‚¹ëœ ì±„ë¡œ í•™ìŠµí•˜ë©´ ì‹¤ì œ ë°ì´í„°ëŠ” ë§ˆìŠ¤í‚¹ë˜ì–´ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ê´´ë¦¬ê°€ ìˆëŠ” ê²°ê³¼ê°€ ë‚˜ì˜¤ê¸°ë•Œë¬¸ì—, ì‹¤ì œë¡œëŠ” ë§ˆìŠ¤í‚¹í•˜ê¸°ë¡œ í•œ ë‹¨ì–´ì˜ 80%ëŠ” ë§ˆìŠ¤í‚¹, 10%ëŠ” ì„ì˜ì˜ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ êµì²´, 10%ëŠ” ê·¸ëŒ€ë¡œ ë‘ëŠ” í˜•ì‹ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ë‹¤.</p>

<p><strong>Next Sentence Prediction (NSP)</strong></p>

<p>ë¬¸ì¥ ê°„ì˜ ê´€ê³„ì— ëŒ€í•œ ì˜ˆì¸¡í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê²ƒ, BERTì˜ Pre-Training í•™ìŠµ ë°©ë²• ì¤‘ í•˜ë‚˜</p>

<ul>
  <li>ì–´ë–¤ ë¬¸ì¥ì´ íŠ¹ì • ë¬¸ì¥ ë‹¤ìŒ ë¬¸ì¥ìœ¼ë¡œ ì˜¬ë§Œí•œ ë¬¸ì¥ì¸ê°€? ì—°ê´€ìˆëŠ” ë¬¸ì¥ì¸ê°€? ì•„ë‹ˆë©´ ê·¸ëƒ¥ ëœë¤ ë¬¸ì¥ì¸ê°€?</li>
  <li>ì•„ë˜ì™€ ê°™ì´ dataë¥¼ ìƒì„±í•˜ì—¬ êµìœ¡ ì‹œí‚¨ë‹¤.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
Label = IsNext

Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
Label = NotNext

</code></pre></div></div>
<p><strong>[code. NSP Input ì˜ˆì œ ]</strong></p>

<ul>
  <li>[CLS] í† í° : GPTì˜ Extractor í† í°, ë¬¸ì¥ì˜ ì˜ˆì¸¡ì„ ìœ„í•œ ì„ ì–¸ìš©?</li>
  <li>[SEP] í† í° : ë¬¸ì¥ ì‚¬ì´ì— êµ¬ë³„ì„ ìœ„í•œ í† í°, ë¬¸ì¥ì˜ ëì„ ì•Œë¦¼</li>
  <li>Label : í¬í•¨ëœ ë‘ ë¬¸ì¥ì˜ ê´€ê³„ì— ëŒ€í•œ Label</li>
</ul>

<p><strong>Summary</strong></p>

<p>êµ¬ì¡°ëŠ” Transform êµ¬ì¡°ì™€ ìœ ì‚¬í•˜ë©°,</p>

<p>BERT BASEì˜ ê²½ìš° SELF-ATTENTION LAYER 12ê°œ, ATTENTION HEAD 12ê°œ,  ì¸ì½”ë”© ë²¡í„° ì°¨ì› ìˆ˜ëŠ” 768ê°œì´ë©°, ê²½ëŸ‰í™”ë˜ì–´ ìˆìœ¼ë©°,</p>

<p>BERT LARGETì˜ ê²½ìš° SELF-ATTENTION LAYER 24ê°œ, ATTENTION HEAD 16ê°œ,  ì¸ì½”ë”© ë²¡í„° ì°¨ì› ìˆ˜ëŠ” 1024ê°œ</p>

<p>INPUT SEQUENCEì˜ ê²½ìš°</p>

<ol>
  <li>
    <p>ë‹¨ì–´ë¥¼ SUBWORDë¼ëŠ” ë‹¨ìœ„ë¡œ ì˜ê°œ ìª¼ê°œëŠ” WordPiece embeddings ë¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.(ex) Pretraining -&gt; pre, training)</p>
  </li>
  <li>
    <p>ì•ì„œ ì„¤ëª…í–ˆë˜ Poisitional Encoding ë°©ë²•ì„ ë³€í˜•í•œ ë¯¸ë¦¬ í•™ìŠµí•œ ìµœì í™”ëœ ê°’(Learned positional embedding)ì„ ì´ìš©í•œë‹¤.</p>
  </li>
  <li>[CLS](Classification embedding), [SEP](Packed sentence embedding) TOKEN</li>
  <li>Segment Embedding</li>
</ol>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221104508746.png" alt="" /></p>

<p><strong>[img. Segment embeddingì˜ ì‚¬ìš©]</strong></p>

<ul>
  <li>ì¼ì¢…ì˜ ë¬¸ì¥ë³„ë¡œ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ì†Œì† ë¬¸ì¥ ì†ì„±ì´ ì¶”ê°€ëœ Embedding</li>
  <li>Token + Segment + Position Vectorë¥¼ ì„œë¡œ ë”í•´ì¤€ë‹¤.</li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221110537068.png" alt="" /></p>

<p><strong>[img. BERTì˜ Transfer Learningì„ ìœ„í•œ Fine-Tuning ê³¼ì •]</strong></p>

<ul>
  <li>Input Layerì™€ Output Layerì„ ë‹¬ë¦¬í•˜ì—¬ íŠ¹ì • Downstream Taskë¥¼ ìœ„í•œ ëª¨ë¸ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.</li>
  <li>Masked tokenì˜ predictionì„ ìœ„í•œ Layerë¥¼ ì œê±°í•œ í›„, ìš°ë¦¬ Taskë¥¼ ìœ„í•œ Layerë¡œ ë°”ê¾¼ ë’¤, ê¸°í•™ìŠµëœ Transfomer encoderì˜ parameterë“¤ì€ ì‘ì€ Learning rateë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡°ê¸ˆë§Œ í•™ìŠµì´ ë˜ê²Œ í•œë‹¤.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Â </th>
      <th>BERT</th>
      <th>GPT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Training-data size</td>
      <td>BookCorpus + Wikipedia(2,500M words)</td>
      <td>BookCorpus(800M words)</td>
    </tr>
    <tr>
      <td>Training special tokens during training</td>
      <td>[SEP],[CLS], sentence A/B embedding during pre-training</td>
      <td>-</td>
    </tr>
    <tr>
      <td>Batch size</td>
      <td>128,000 words</td>
      <td>32,000 words</td>
    </tr>
    <tr>
      <td>Task-specific fine-tuning</td>
      <td>task-specific fine-tuning learning rate(task ë³„ë¡œ ë‹¤ë¦„)</td>
      <td>5e-5 ê³ ì •</td>
    </tr>
    <tr>
      <td>Â </td>
      <td>Masked tokenë¥¼ í†µí•´ ì•ë’¤ ëª¨ë“  ë‹¨ì–´ ì ‘ê·¼ ê°€ëŠ¥</td>
      <td>ì˜¤ì§ ì´ë¯¸ ë‚˜ì˜¨ ë‹¨ì–´ë§Œ ì ‘ê·¼ ê°€ëŠ¥</td>
    </tr>
  </tbody>
</table>

<p><strong>[fig. BERT vs GPT]</strong></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221111445326.png" alt="" /></p>

<p><strong>[img. GLUE data, BERT ì„±ëŠ¥ ë¹„êµ]</strong></p>

<p>Machine Reading Comprehension(MRC)ë€, ê¸°ê³„ë…í•´í•˜ì—¬ ì§ˆì˜ì‘ë‹µ(Question Answering)í•˜ëŠ” Task</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Daniel and Sandra journyed to the office, Then they went to the garden. Sandra and John travelled to the kitchen. After that they moved to the hallway.

q: where is daniel?
a: garden

</code></pre></div></div>
<p><strong>[code. MRC ì˜ˆì œ]</strong></p>

<p>SQuAD ê°™ì€ MRC ë°ì´í„°ì…‹ì—ì„œ BERT ëª¨ë¸ì€ ì–¸ì œë‚˜ ìˆœìœ„ê¶Œì— ë§ì´ ì¡´ì¬í•œë‹¤.</p>

<ol>
  <li>SQuAD 1.1</li>
</ol>

<ul>
  <li>ì§ˆë¬¸ê³¼ ë°ì´í„°ì…‹ì„ sentenceë¡œ ìƒê°í•˜ê³  [CLS] í† í°ìœ¼ë¡œ ë¶™ì¸ í›„,  ë‹¨ì–´ë“¤ì„ Fully connected layerë¥¼ í†µí•´ ìŠ¤ì¹¼ë¼ ê°’ì„ ì–»ì–´ë‚¸ í›„, ë‹µë³€ì´ ì¡´ì¬í•˜ëŠ” ë¬¸êµ¬ì˜ ì‹œì‘ ìœ„ì¹˜ ë‹¨ì–´ì™€ ë ìœ„ì¹˜ ë‹¨ì–´ë¥¼ ì°¾ì•„ë‚´ëŠ” ë°©ì‹</li>
</ul>

<ol>
  <li>SQuAD 2.0 : BERT ë³€í˜• ëª¨ë¸ì´ ë§ì´ ì¡´ì¬</li>
</ol>

<ul>
  <li>toekn 0 <span class="preview-wrapper">
      <a href="" class="wikilink">CLS</a>
      <iframe class="preview" src='' data-href="" style="display:none;" sandbox="allow-same-origin allow-scripts">
      </iframe>
    </span> í† í°ì„ no answer ë¡œ ì‚¬ìš© í•˜ì—¬ ë‹µì´ ì—†ëŠ” ê²½ìš°ë„ ë°©ì§€</li>
</ul>

<ol>
  <li>On SWAG</li>
</ol>

<ul>
  <li>ì§ˆë¬¸ + ì„ íƒì§€ë“¤ì„ [CLS] í† í°ê³¼ í•¨ê»˜ ê°ê° concatí•œ ë’¤, ë‚˜ì˜¤ëŠ” Encoding vectorë¥¼ Fully connected layerì„ í†µê³¼í•´ì„œ ë‚˜ì˜¨ Scalar ê°’ìœ¼ë¡œ íŒë‹¨.</li>
</ul>

<ol>
  <li>Ablation Study</li>
</ol>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221122235801.png" alt="" /></p>

<p><strong>[img. Model Size vs ì„±ëŠ¥]</strong></p>

<ul>
  <li>Modelì˜ Parameter í¬ê¸°ê°€ í´ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ì•„ì§„ë‹¤.</li>
</ul>

<h3 id='Advanced-Self-supervised-Pre-training-Models'>Advanced Self-supervised Pre-training Models</h3>

<ul>
  <li>ì¢€ë” ê²½ëŸ‰í™”, ê³ ë„í™”ëœ Pre-training Model</li>
</ul>

<h4 id='GPT-2'>GPT-2</h4>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221151857530.png" alt="" /></p>

<p><strong>[img. GPT-2 ê²°ê³¼ ì˜ˆì‹œ]</strong></p>

<p>GPT-1ê³¼ êµ¬ì¡°ê°€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šìœ¼ë©°, layerê°€ ë” ì‹¶ê³  training dataê°€ ë” ë§ì•„ì§„ í˜•íƒœ</p>

<ul>
  <li>ì¼ë¶€ Layer normlizationì˜ ìœ„ì¹˜ ì´ë™</li>
  <li>residual layerì˜ weightë¥¼ residual layerì˜ ìˆ˜ ë£¨íŠ¸ê°’ì— ë°˜ë¹„ë¡€í•˜ê²Œ ì¤„ì—¬ì¤Œ.(=ì˜í–¥ë ¥ ì•½í™”)</li>
  <li><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221153525328.png" alt="" /></li>
</ul>

<p>ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í•™ìŠµì‹œí‚´, dataset í€¼ë¦¬í‹°ë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ textë“¤ì„ ê±¸ë €ìŒ</p>

<ul>
  <li>Redditì´ë¼ëŠ” ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì¢‹ì•„ìš”ê°€ 3ê°œ ì´ìƒì¸ ì™¸ë¶€ë§í¬ì˜ documentë¥¼ ìˆ˜ì§‘</li>
  <li>ì „ì²˜ë¦¬ë¡œ Bytpe pair encoding ì ìš©, subword ìˆ˜ì¤€ì˜ word embedding.
    <ul>
      <li>êµ¬í˜„ ë°©ë²•ì€ [NLP-Assignment]Byte_Pair_Encoding,ipynb ì°¸ì¡°</li>
    </ul>
  </li>
</ul>

<p>down-stream taskì—ì„œ zero-shot setting, ì¶”ê°€ parameterë‚˜ êµ¬ì¡° ë³€ê²½ì—†ì´ ì‚¬ìš© ê°€ëŠ¥</p>

<p>decaNLPê°€ ë™ê¸°ê°€ ë¨. : ëª¨ë“  language taskëŠ” Question Answeringìœ¼ë¡œ í†µí•©í•  ìˆ˜ ìˆë‹¤.</p>

<ul>
  <li>ê¸°ê³„ ë²ˆì—­: ì´ ë¬¸ì¥ì˜ í•œêµ­ì–´ ë²„ì „ì€ ë¬´ì—‡ì¸ê°€?</li>
  <li>ë¬¸ì„œ ìš”ì•½: ì´ ë¬¸ì„œì˜ ìš”ì ì€ ë¬´ì—‡ì¸ê°€?</li>
  <li>ê°ì • ë¶„ì„: ê·¸ë˜ì„œ í™”ìëŠ” ì´ ë¬¼ê±´ì„ ì¢‹ì•„í•˜ëŠ”ê°€? ì‹«ì–´í•˜ëŠ”ê°€?</li>
  <li>ì´ ì›ë¦¬ë¥¼ í†µí•´ zero-shot setting, ë³„ë‹¤ë¥¸ ëª¨ë¸ë³€ê²½, í•™ìŠµ(fine-tuning) ì—†ì´ down-stream task ê°€ëŠ¥,</li>
</ul>

<h5 id='GPT-2-ì˜ˆì‹œ'>GPT-2 ì˜ˆì‹œ</h5>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221155332976.png" alt="" /></p>

<p><strong>[img. GPT-2 ë¬¸ì„œ ìš”ì•½]</strong></p>

<p>ë³„ë‹¤ë¥¸ fine-tuning ì—†ì´ ë¬¸ì„œ ë§ˆì§€ë§‰ì— TL;DR:(Too long, didnâ€™t read) ì´ë€ ë‹¨ì–´ë¥¼ ì¶”ê°€í•˜ë©´ ë‹¤ìŒì— ìš”ì•½í•œ ë¬¸ì¥ì„ ìƒì„±í•´ì¤Œ.</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221155510723.png" alt="" /></p>

<p><strong>[img. GPT-2 ê¸°ê³„ ë²ˆì—­]</strong></p>

<ul>
  <li>ë¬¸ì¥ì— wrote in French: ë¼ê³  ë¶™ì´ì ë¶ˆì–´ë¡œ ë²ˆì—­í•´ì¤Œ</li>
</ul>

<h4 id='GPT-3'>GPT-3</h4>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221155700637.png" alt="" /></p>

<p><strong>[img. GTP-3 parameter ìˆ˜ ë¹„êµ]</strong></p>

<p>GPT-2ì— ë¹„í•´ ëª¨ë¸ êµ¬ì¡° ë“±ì— íŠ¹ë³„í•œ ë³€í™”ë³´ë‹¤, Parameter ìˆ˜, Dataset, Batch sizeë¥¼ ë¹„êµí•  ìˆ˜ ì—†ì„ ì •ë„ë¡œ í¬ê²Œ ë°”ê¿”ì¤Œ</p>

<p>96 Attention layers, Batch size of 3.2M</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221164110720.png" alt="" /></p>

<p><strong>[img. shot]</strong></p>

<p>Zero-shot settingìœ¼ë¡œë„ ì¶©ë¶„íˆ downstream-taskê°€ ê°€ëŠ¥í•˜ì§€ë§Œ one-shot, Few-shot ì²˜ëŸ¼ exampleì„ ì¤Œìœ¼ë¡œ ë”ìš± ì„±ëŠ¥ì„ í–¥ìƒí•  ìˆ˜ ìˆë‹¤.</p>

<ul>
  <li>Zero-shot: Question answering ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì¥ì„ ì£¼ê³  ë²ˆì—­</li>
  <li>One-shot: í•˜ë‚˜ì˜ ë²ˆì—­ ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ê³  ë²ˆì—­</li>
  <li>Few-shot: ì—¬ëŸ¬ ê°œì˜ ë²ˆì—­ ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ê³  ë²ˆì—­</li>
  <li>zero-shotìœ¼ë¡œë„ model parameterì˜ ìˆ˜ê°€ ì»¤ì§ˆ ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¦ê°€í•˜ì§€ë§Œ,  shotì´ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ê¸‰ê²©í•˜ê²Œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§.</li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221170455928.png" alt="" /></p>

<p><strong>[img. shotë³„ ì„±ëŠ¥ í–¥ìƒ]</strong></p>

<h4 id='ALBERT'>ALBERT</h4>

<p>ê²½ëŸ‰í™”ëœ BERT, Lite BERTë¼ëŠ” ì˜ë¯¸</p>

<p>ì ì  ëª¨ë¸ì´ ê±°ëŒ€í•´ì§€ë©´ì„œ ë©”ëª¨ë¦¬ì˜ í•œê³„ì™€ ëŠë ¤ì§„ í•™ìŠµ ì†ë„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì„ ì´ìš©í–ˆë‹¤.</p>

<ul>
  <li>
    <p>Factorized Embedding Parametrization</p>

    <p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221171147925.png" alt="" /></p>

    <p><strong>[img. Factorized Embedding Parametraiztion]</strong></p>

    <p>Residual connectionì€ skipí•˜ë©° layer ì‚¬ì´ë§ˆë‹¤ ë”í•´ì§€ë¯€ë¡œ ì¼ì •í¬ê¸°ì˜ dimensionì˜ ìœ ì§€ê°€ ê°•ì œëœë‹¤.</p>

    <p>ì´ dimensionì´ ë„ˆë¬´ ì‘ìœ¼ë©´, ì •ë³´ê°€ ë§ì´ ì•ˆë‹´ê¸°ë©°, ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´  Parameter ìˆ˜ê°€ ëŠ˜ì–´ë‚˜ê³ , ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦°ë‹¤.</p>

    <p>ALBERTì—ì„œëŠ” embeddingì˜ ì°¨ì› í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œ ì •ë³´ì˜ ì†Œì‹¤ì„ ì¤„ì´ê³ , Residual í•©ì´ ê°€ëŠ¥í•œ ë°©ë²•ì„ ì“´ë‹¤.</p>

    <p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221192141795.png" alt="" /></p>

    <p><strong>[img. BERT VS ALBERT]</strong></p>

    <p>â€‹	- V : Vocabulary size, H : Hidden-state dimension, E : Word embedding dimension</p>

    <ul>
      <li>ALBERTëŠ” ì‘ì€ í¬ê¸°ì˜ word embeddingì— ì¶”ê°€ë¡œ dimensionì„ ëŠ˜ë ¤ì£¼ëŠ” layerì„ ì¶”ê°€í•˜ì—¬ Residual net ì´ì „ì— ì°¨ì› í¬ê¸°ë¥¼ ë¶ˆë ¤ì„œ í•´ê²°í–ˆë‹¤.  (Row rank matrix factorization)</li>
    </ul>
  </li>
  <li>
    <p>Cross-layer Parameter Sharing</p>

    <ul>
      <li>
        <p>Multihead-self-attention êµ¬ì¡°ëŠ” ê°ê° headë§ˆë‹¤ í•™ìŠµì‹œì¼œì•¼í•  ì„ í˜•ë³€í™˜ í–‰ë ¬($W_Q,W_V,W_K,W_o \times $ head ìˆ˜ ë“±)ë“¤ì´ ì¡´ì¬í•˜ëŠ”ë°, ì´ë¥¼ headë§ˆë‹¤ êµ¬ë¶„í•˜ì§€ ë§ê³  ê³µìœ í•˜ëŠ” í•˜ë‚˜ì˜ í–‰ë ¬ë¡œ ë°”ê¾¸ëŠ” êµ¬ì¡°ë¥¼ ì˜ë¯¸</p>
      </li>
      <li>
        <p>Shared-FFN : feed-forward network parameterë§Œ ê³µìœ </p>
      </li>
      <li>
        <p>Shared-attention : attention layerì˜ parameter ê³µìœ </p>
      </li>
      <li>
        <p>All-shared: ìœ„ ë‘ê°œ ì „ë¶€ ê³µìœ </p>

        <p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221203810848.png" alt="" /></p>

        <p><strong>[img. share í–ˆì„ ì‹œì— ì„±ëŠ¥ ë¹„êµ, ParameterëŠ” í¬ê²Œ ì¤„ì–´ë“¤ê³ , ì„±ëŠ¥ì€ ì¡°ê¸ˆ í•˜ë½]</strong></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Sentence Order Prediction</p>

    <ul>
      <li>BERTì˜ Next Sentence Prediction taskê°€ ë¹„íš¨ìœ¨ì ì´ë¼ íŒë‹¨í•˜ì—¬ Masked Language Modelë§Œ ì‹¤ì‹œ</li>
      <li>ëŒ€ì‹  ê°™ì€ ë¬¸ì„œì—ì„œ 2ê°œì˜ ë¬¸ì¥ì˜ ìˆœì„œë¥¼ ë’¤ë°”ê¾¸ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘” ë’¤, ì •ìƒì ì¸ ìˆœì„œì¸ì§€ ë§ì¶”ëŠ” í•™ìŠµ(Sentence Order Prediction)ì„ ì‹¤ì‹œí•¨.
        <ul>
          <li>BERTì˜ NSPëŠ” ë‹¤ë¥¸ ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ Negative sampleì˜ ê²½ìš°, ë„ˆë¬´ ì‰½ê³ , ê°™ì€ ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ Postive sampleì˜ ê²½ìš° ë¬¸ë§¥ì´ ì•„ë‹ˆë¼, ë‹¨ì–´ì˜ ë¹ˆë„ ë“±ìœ¼ë¡œ ë§ì¶”ëŠ” ë“±ì˜ ë¬¸ì œê°€ ìˆì—ˆìŒ</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221204728460.png" alt="" /></p>

    <p><strong>[img. ê¸°ì¡´ BERTì™€ ë¹„êµì‹œ, ì˜¤íˆë ¤ SOPì˜ ì„±ëŠ¥ì´ ê°€ì¥ ë†’ë‹¤.]</strong></p>
  </li>
</ul>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221205256017.png" alt="" /></p>

<p><strong>[img. GLUE ì„±ëŠ¥ ë¹„êµ ê²°ê³¼]</strong></p>

<ul>
  <li>ALBERT ì„±ëŠ¥ì´ ì¢‹ë‹¤.</li>
</ul>

<h4 id='ELECTRA'>ELECTRA</h4>

<p>Efficiently Learning an Encoder that Classifies Token Replacements Accuratelyì˜ ì¤€ë§</p>

<p>BERT, GPTì˜ í•™ìŠµ ë°©ì‹ê³¼ ë‹¬ë¦¬ Generoatorë€ ë‹¨ì–´ ë³µì›ê¸°ë¥¼ í†µí•´ Maskingëœ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ë³µì›í•œ ë’¤,  ê·¸ ê²°ê³¼ë¥¼ ELECTRAì˜ Discriminatorë¥¼ í†µí•˜ì—¬ ì›ë³¸ê³¼ ë¹„êµí•˜ì—¬, ë³µì›ëœ ë‹¨ì–´ì¸ì§€, ì›ë˜ masking ì•ˆëœ ì›ë³¸ì¸ì§€ êµ¬ë³„í•˜ëŠ” ë°©ë²•(GAN, +generative adversal network)ì´ë‹¤.</p>

<p>í•™ìŠµì€ Generator, Discriminaotr ë‘˜ë‹¤, ì‹¤ì œ test ì—ëŠ” Discriminatorë§Œ ì‚¬ìš©</p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221205934821.png" alt="" /></p>

<p><strong>[img. ELECTRAì˜ í•™ìŠµ ë°©ë²•]</strong></p>

<p><img src="/assets/img/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸/image-20210221211534841.png" alt="" /></p>

<p><strong>[img. ELECTRAì™€ ë‹¤ë¥¸ ëª¨ë¸ ë¹„êµ]</strong></p>

<ul>
  <li>í•™ìŠµ ì‹œí‚¬ìˆ˜ë¡ ë‹¤ë¥¸ ëª¨ë¸ì— ë¹„í•´ ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
</ul>

<h4 id='Light-weight-Models'>Light-weight Models</h4>

<p>ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ì´ ë„ˆë¬´ ë¬´ê±°ì›Œì„œ ê°€ë³ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ê°€ì„±ë¹„ ëª¨ë¸, ì„ë² ë”© ë¨¸ì‹ , ìŠ¤ë§ˆíŠ¸ í° ë“±ì— ì‚¬ìš©</p>

<ul>
  <li>DistillBERT
    <ul>
      <li>Hugging faceì˜ ë…¼ë¬¸</li>
      <li>Teacher-student ëª¨ë¸
        <ul>
          <li>Teacher modelì€ íŒŒë¼ë¯¸í„°, ë ˆì´ì–´ ìˆ˜ê°€ ë¹„êµì  ë”í¬ë©° studentê°€ Teacherì˜ ê²°ê³¼(softmax ê²°ê³¼)ì„ ëª¨ë°©í•˜ë„ë¡ í•¨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>TinyBERT
    <ul>
      <li>Teacher-student modelê³¼ ë¹„ìŠ·í•˜ë‚˜ parameterì™€ hidden state ë“±, ì¤‘ê°„ ê²°ê³¼ ê¹Œì§€ë„ ë¹„ìŠ·í•˜ê²Œ í•˜ë„ë¡ ëª¨ë°©í•˜ëŠ” ëª¨ë¸</li>
    </ul>
  </li>
</ul>

<h4 id='Fusing-Knowledge-Graph-into-Language-Model'>Fusing Knowledge Graph into Language Model</h4>

<ul>
  <li>ì£¼ì–´ì§„ ë¬¸ì¥ ë¿ë§Œ ì•„ë‹ˆë¼ ìƒì‹ ë˜ëŠ” ì™¸ë¶€ì§€ì‹(Knowledge Graph)ì—ì„œ ì§€ì‹ì„ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•</li>
  <li>ERNIE</li>
  <li>KagNET</li>
</ul>

</div>

  </div><a class="u-url" href="/articles/AI/NLP/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%B3%B8.html" hidden></a>
  <p class="u-path" hidden>_articles/AI/NLP/ìì—°ì–´ì²˜ë¦¬ ê¸°ë³¸.md</p>
  <script type="module" src="/assets/scripts/utils/update_recents.js"></script>
</article>

    </div>
  </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The Digital garden of Nurgle.</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="a-name">The Digital garden of Nurgle.</li><li><a class="u-email" href="mailto:roadvirushn@gmail.com">roadvirushn@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li>
    <a href="https://github.com/RoadVirusHN"><svg class="svg-icon">
        <use xlink:href="/assets/svg/social-icons.svg#github"></use>
      </svg>
      <span class="username">RoadVirusHN</span></a>
  </li><!---->
</ul></div>

      <div class="footer-col footer-col-3">
        <p>ì´ê²ƒì´ ë””ì§€í„¸ ë™ë¬¼ì˜ ìˆ²ì´ë‹¤!! íŒŒë©¸í¸ (This is the Digital Animal Crossing!! Bad Ending.01)</p>
      </div>
    </div>

  </div>

</footer>
</body>

<script src="/assets/scripts/bundle/common.bundle.js"></script>

</html>